{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import sounddevice as sd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import whisperx\n",
    "import gc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de963d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../datasets/EDAIC-WOZ\"\n",
    "\n",
    "# Remove folders from 300 to 492\n",
    "for i in range(300, 493):\n",
    "    folder_name = f\"{i}_P\"\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "\n",
    "# XXX_P/XXX_P -> XXX_P\n",
    "for dir_name in os.listdir(base_dir):\n",
    "    outer_path = os.path.join(base_dir, dir_name)\n",
    "    if os.path.isdir(outer_path):\n",
    "        inner_path = os.path.join(outer_path, dir_name)\n",
    "        if os.path.isdir(inner_path):\n",
    "            # Sposta tutti i file dal secondo livello al primo\n",
    "            for filename in os.listdir(inner_path):\n",
    "                src = os.path.join(inner_path, filename)\n",
    "                dst = os.path.join(outer_path, filename)\n",
    "                shutil.move(src, dst)\n",
    "            # Rimuove la cartella interna vuota\n",
    "            os.rmdir(inner_path)\n",
    "\n",
    "# Csv files to concatenate\n",
    "csv_files = ['dev_split.csv', 'test_split.csv', 'train_split.csv']\n",
    "dfs = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    path = os.path.join(base_dir, csv_file)\n",
    "    df = pd.read_csv(path)\n",
    "    dfs.append(df)\n",
    "\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Filter from 300 to 492\n",
    "all_data = all_data[~all_data['Participant_ID'].between(300, 492)]\n",
    "\n",
    "# Save\n",
    "output_path = os.path.join(base_dir, \"all_data.csv\")\n",
    "all_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba26e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix inconsistent PHQ labels\n",
    "# Find participants with PHQ_Score >= 10 but PHQ_Binary = 0\n",
    "inconsistent_mask = (all_data['PHQ_Score'] >= 10) & (all_data['PHQ_Binary'] == 0)\n",
    "inconsistent_participants = all_data[inconsistent_mask]['Participant_ID'].tolist()\n",
    "\n",
    "print(f\"Found {len(inconsistent_participants)} participants with inconsistent PHQ labels:\")\n",
    "for participant_id in inconsistent_participants:\n",
    "    phq_score = all_data[all_data['Participant_ID'] == participant_id]['PHQ_Score'].iloc[0]\n",
    "    print(f\"  Participant {participant_id}: PHQ_Score={phq_score}, PHQ_Binary=0 -> fixing to PHQ_Binary=1\")\n",
    "\n",
    "# Fix the inconsistent labels\n",
    "all_data.loc[inconsistent_mask, 'PHQ_Binary'] = 1\n",
    "\n",
    "print(f\"\\nFixed {len(inconsistent_participants)} inconsistent labels\")\n",
    "\n",
    "# Save the corrected data\n",
    "all_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete directories for non-depressed participants (PHQ_Binary = 0)\n",
    "non_depressed_participants = all_data[all_data['PHQ_Binary'] == 0]['Participant_ID'].unique()\n",
    "\n",
    "print(f\"Found {len(non_depressed_participants)} non-depressed participants to remove:\")\n",
    "\n",
    "deleted_count = 0\n",
    "for participant_id in non_depressed_participants:\n",
    "    folder_name = f\"{participant_id}_P\"\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        deleted_count += 1\n",
    "\n",
    "print(f\"Deleted {deleted_count} directories for non-depressed participants\")\n",
    "\n",
    "# Remove non-depressed participants from the CSV data\n",
    "all_data = all_data[all_data['PHQ_Binary'] == 1]\n",
    "all_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e97619",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "compute_type = \"float16\" if torch.cuda.is_available() else \"float32\"\n",
    "batch_size=32\n",
    "model_id = \"tiny.en\"\n",
    "language = \"en\"\n",
    "temp_dir = os.path.join(base_dir, \"temp_results\")\n",
    "os.makedirs(temp_dir, exist_ok=True) \n",
    "\n",
    "sessions = sorted([d for d in os.listdir(base_dir) \\\n",
    "                   if os.path.isdir(os.path.join(base_dir, d)) and d.endswith('_P')])\n",
    "sessions = sessions[:1]\n",
    "\n",
    "model = whisperx.load_model(model_id, device, compute_type=compute_type, language=language)\n",
    "\n",
    "for session in tqdm(sessions, desc=\"Trascrizione Audio\"):\n",
    "    session_path = os.path.join(base_dir, session)\n",
    "    base_name = session.split(\"_\")[0]\n",
    "    audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "    intermediate_path = os.path.join(temp_dir, f\"{session}_transcript.json\")\n",
    "    if not os.path.exists(audio_path):\n",
    "        print(f\"Audio non trovato: {audio_path}. Salto la sessione.\")\n",
    "        continue\n",
    "    print(f\"\\nSto processando: {audio_path}\")\n",
    "    \n",
    "    audio = whisperx.load_audio(audio_path)\n",
    "    result = model.transcribe(audio, batch_size=batch_size)\n",
    "    print(result)\n",
    "\n",
    "    with open(intermediate_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Trascrizione intermedia salvata in: {intermediate_path}\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
    "\n",
    "for session in tqdm(sessions, desc=\"Allineamento Audio\"):\n",
    "    intermediate_path = os.path.join(temp_dir, f\"{session}_transcript.json\")\n",
    "    if not os.path.exists(intermediate_path):\n",
    "        print(f\"File intermedio non trovato: {intermediate_path}. Salto.\")\n",
    "        continue\n",
    "    print(f\"\\nSto allineando: {session}\")\n",
    "\n",
    "    # Carica il risultato della trascrizione\n",
    "    with open(intermediate_path, 'r', encoding='utf-8') as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    # Se la trascrizione è vuota, salta l'allineamento\n",
    "    if not result[\"segments\"]:\n",
    "        print(\"Nessun segmento da allineare.\")\n",
    "        continue\n",
    "        \n",
    "    # Per l'allineamento è necessario ricaricare l'audio\n",
    "    session_path = os.path.join(base_dir, session)\n",
    "    base_name = session.split(\"_\")[0]\n",
    "    audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "    audio = whisperx.load_audio(audio_path)\n",
    "    \n",
    "    # Esegui l'allineamento\n",
    "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "    print(result)\n",
    "\n",
    "    # Aggiorna il file intermedio con i dati di allineamento\n",
    "    with open(intermediate_path, 'w', encoding='utf-8') as f:\n",
    "        # result ora contiene 'word_segments', ma lo salviamo sotto la chiave 'segments'\n",
    "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"Allineamento salvato in: {intermediate_path}\")\n",
    "\n",
    "del model_a\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "load_dotenv()\n",
    "diarize_model = whisperx.diarize.DiarizationPipeline(\n",
    "    use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"), \n",
    "    device=device\n",
    ")\n",
    "\n",
    "for session in tqdm(sessions, desc=\"Diarizzazione Speaker\"):\n",
    "    session_path = os.path.join(base_dir, session)\n",
    "    base_name = session.split(\"_\")[0]\n",
    "    audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "    intermediate_path = os.path.join(temp_dir, f\"{session}_transcript.json\")\n",
    "    transcript_path = os.path.join(session_path, f\"{base_name}_TRANSCRIPT.csv\")\n",
    "    \n",
    "    if not os.path.exists(intermediate_path):\n",
    "        print(f\"File intermedio non trovato: {intermediate_path}. Salto.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nSto diarizzando: {session}\")\n",
    "    \n",
    "    # Carica il risultato allineato\n",
    "    with open(intermediate_path, 'r', encoding='utf-8') as f:\n",
    "        result = json.load(f)\n",
    "\n",
    "    # Se non ci sono segmenti, crea un CSV vuoto\n",
    "    if not result[\"segments\"]:\n",
    "        print(f\"Nessun segmento rilevato in {audio_path}. Salvataggio di un transcript vuoto.\")\n",
    "        pd.DataFrame(columns=[\"start_time\", \"stop_time\", \"speaker\", \"value\"]).to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "        continue\n",
    "\n",
    "    # Esegui la diarizzazione sull'audio completo\n",
    "    audio = whisperx.load_audio(audio_path)\n",
    "    diarize_segments = diarize_model(audio, min_speakers=2, max_speakers=4) \n",
    "\n",
    "    # Assegna gli speaker alle parole\n",
    "    result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "    print(result)\n",
    "\n",
    "    # Raggruppa le parole in segmenti per speaker\n",
    "    final_segments = []\n",
    "    current_segment = None\n",
    "    all_words = []\n",
    "    for segment in result[\"segments\"]:\n",
    "        all_words.extend(segment.get(\"words\", []))\n",
    "\n",
    "    if not all_words:\n",
    "        print(f\"Nessuna parola trovata dopo l'allineamento per {session}. Salto la creazione dei segmenti.\")\n",
    "        pd.DataFrame(columns=[\"start_time\", \"stop_time\", \"speaker\", \"value\"]).to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "        continue\n",
    "\n",
    "    for word_info in all_words:\n",
    "        if 'speaker' not in word_info or 'start' not in word_info or 'end' not in word_info:\n",
    "            continue\n",
    "\n",
    "        speaker = word_info[\"speaker\"]\n",
    "        \n",
    "        if current_segment is None:\n",
    "            current_segment = {\"start_time\": word_info[\"start\"], \"speaker\": speaker, \"value\": word_info[\"word\"]}\n",
    "        elif speaker != current_segment[\"speaker\"]:\n",
    "            current_segment[\"stop_time\"] = last_word_end\n",
    "            final_segments.append(current_segment)\n",
    "            current_segment = {\"start_time\": word_info[\"start\"], \"speaker\": speaker, \"value\": word_info[\"word\"]}\n",
    "        else:\n",
    "            current_segment[\"value\"] += \" \" + word_info[\"word\"]\n",
    "\n",
    "        last_word_end = word_info[\"end\"]\n",
    "\n",
    "    if current_segment is not None:\n",
    "        current_segment[\"stop_time\"] = last_word_end\n",
    "        final_segments.append(current_segment)\n",
    "    \n",
    "    # Creazione e salvataggio del DataFrame finale\n",
    "    df = pd.DataFrame(final_segments)\n",
    "    if not df.empty:\n",
    "        df = df[[\"start_time\", \"stop_time\", \"speaker\", \"value\"]]\n",
    "    else:\n",
    "        print(f\"Non è stato possibile creare segmenti finali per {session}. Salvo un file vuoto.\")\n",
    "        df = pd.DataFrame(columns=[\"start_time\", \"stop_time\", \"speaker\", \"value\"])\n",
    "\n",
    "    df.to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Output finale salvato in: {transcript_path}\")\n",
    "\n",
    "    # (Opzionale) Rimuovi il file intermedio dopo l'uso\n",
    "    # os.remove(intermediate_path)\n",
    "\n",
    "del diarize_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio_segment(audio_tensor, sample_rate):\n",
    "    # audio_tensor shape: [channels, samples]\n",
    "    audio_np = audio_tensor.numpy().T  # Trasponi per shape (samples, channels)\n",
    "    sd.play(audio_np, sample_rate)\n",
    "    sd.wait()\n",
    "\n",
    "def label_speakers(audio_path, transcript_path):\n",
    "    df = pd.read_csv(transcript_path, sep=\"\\t\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    speaker_labels = {}\n",
    "    speakers = df['speaker'].unique()\n",
    "\n",
    "    for spk in speakers:\n",
    "        first_seg = df[df['speaker'] == spk].iloc[0]\n",
    "        start_sample = int(first_seg['start_time'] * sample_rate)\n",
    "        end_sample = int(first_seg['stop_time'] * sample_rate)\n",
    "\n",
    "        segment_audio = waveform[:, start_sample:end_sample]\n",
    "\n",
    "        print(f\"\\nSpeaker: {spk} - playing audio segment from {start_sample/sample_rate:.2f}s to {end_sample/sample_rate:.2f}s\")\n",
    "        play_audio_segment(segment_audio, sample_rate)\n",
    "\n",
    "        choice = input(\"Label this speaker as (E)llie, (P)articipant, (O)ther: \").strip().lower()\n",
    "        if choice == 'e':\n",
    "            speaker_labels[spk] = \"Ellie\"\n",
    "        elif choice == 'p':\n",
    "            speaker_labels[spk] = \"Participant\"\n",
    "        else:\n",
    "            speaker_labels[spk] = \"ignore\"\n",
    "\n",
    "    df['speaker'] = df['speaker'].map(speaker_labels)\n",
    "    df.to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Updated transcript saved to {transcript_path}\")\n",
    "\n",
    "# Process all sessions for speaker labeling\n",
    "sessions = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "\n",
    "for session in tqdm(sessions, desc=\"Labeling speakers\"):\n",
    "    session_path = os.path.join(base_dir, session)\n",
    "    base_name = session.split(\"_\")[0]\n",
    "    audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "    transcript_path = os.path.join(session_path, f\"{base_name}_TRANSCRIPT.csv\")\n",
    "    \n",
    "    if os.path.exists(audio_path) and os.path.exists(transcript_path):\n",
    "        print(f\"\\n=== Processing session {session} ===\")\n",
    "        try:\n",
    "            label_speakers(audio_path, transcript_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {session}: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Skipping {session}: missing audio or transcript file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
