{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb2e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pyannote.audio import Pipeline\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de963d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"../datasets/EDAIC-WOZ\"\n",
    "\n",
    "# Remove folders from 300 to 492\n",
    "for i in range(300, 493):\n",
    "    folder_name = f\"{i}_P\"\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "\n",
    "# XXX_P/XXX_P -> XXX_P\n",
    "for dir_name in os.listdir(base_dir):\n",
    "    outer_path = os.path.join(base_dir, dir_name)\n",
    "    if os.path.isdir(outer_path):\n",
    "        inner_path = os.path.join(outer_path, dir_name)\n",
    "        if os.path.isdir(inner_path):\n",
    "            # Sposta tutti i file dal secondo livello al primo\n",
    "            for filename in os.listdir(inner_path):\n",
    "                src = os.path.join(inner_path, filename)\n",
    "                dst = os.path.join(outer_path, filename)\n",
    "                shutil.move(src, dst)\n",
    "            # Rimuove la cartella interna vuota\n",
    "            os.rmdir(inner_path)\n",
    "\n",
    "# Csv files to concatenate\n",
    "csv_files = ['dev_split.csv', 'test_split.csv', 'train_split.csv']\n",
    "dfs = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    path = os.path.join(base_dir, csv_file)\n",
    "    df = pd.read_csv(path)\n",
    "    dfs.append(df)\n",
    "\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Filter from 300 to 492\n",
    "all_data = all_data[~all_data['Participant_ID'].between(300, 492)]\n",
    "\n",
    "# Save\n",
    "output_path = os.path.join(base_dir, \"all_data.csv\")\n",
    "all_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba26e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix inconsistent PHQ labels\n",
    "# Find participants with PHQ_Score >= 10 but PHQ_Binary = 0\n",
    "inconsistent_mask = (all_data['PHQ_Score'] >= 10) & (all_data['PHQ_Binary'] == 0)\n",
    "inconsistent_participants = all_data[inconsistent_mask]['Participant_ID'].tolist()\n",
    "\n",
    "print(f\"Found {len(inconsistent_participants)} participants with inconsistent PHQ labels:\")\n",
    "for participant_id in inconsistent_participants:\n",
    "    phq_score = all_data[all_data['Participant_ID'] == participant_id]['PHQ_Score'].iloc[0]\n",
    "    print(f\"  Participant {participant_id}: PHQ_Score={phq_score}, PHQ_Binary=0 -> fixing to PHQ_Binary=1\")\n",
    "\n",
    "# Fix the inconsistent labels\n",
    "all_data.loc[inconsistent_mask, 'PHQ_Binary'] = 1\n",
    "\n",
    "print(f\"\\nFixed {len(inconsistent_participants)} inconsistent labels\")\n",
    "\n",
    "# Save the corrected data\n",
    "all_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete directories for non-depressed participants (PHQ_Binary = 0)\n",
    "non_depressed_participants = all_data[all_data['PHQ_Binary'] == 0]['Participant_ID'].unique()\n",
    "\n",
    "print(f\"Found {len(non_depressed_participants)} non-depressed participants to remove:\")\n",
    "\n",
    "deleted_count = 0\n",
    "for participant_id in non_depressed_participants:\n",
    "    folder_name = f\"{participant_id}_P\"\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        deleted_count += 1\n",
    "\n",
    "print(f\"Deleted {deleted_count} directories for non-depressed participants\")\n",
    "\n",
    "# Remove non-depressed participants from the CSV data\n",
    "all_data = all_data[all_data['PHQ_Binary'] == 1]\n",
    "all_data.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(s1, e1, s2, e2):\n",
    "    return max(0, min(e1, e2) - max(s1, s2))\n",
    "\n",
    "def assign_speakers_to_segments(segments, diarization_result):\n",
    "    output = []\n",
    "    for seg in segments:\n",
    "        start = seg[\"start\"]\n",
    "        end = seg[\"end\"]\n",
    "        text = seg[\"text\"]\n",
    "\n",
    "        speaker = \"UNKNOWN\"\n",
    "        max_ov = 0\n",
    "        for segment, _, label in diarization_result.itertracks(yield_label=True):\n",
    "            ov = overlap(start, end, segment.start, segment.end)\n",
    "            # Abbiamo bisogno di una sovrapposizione minima per considerarla valida\n",
    "            if ov > max_ov:\n",
    "                max_ov = ov\n",
    "                speaker = label\n",
    "\n",
    "        output.append({\n",
    "            \"start_time\": start,\n",
    "            \"stop_time\": end,\n",
    "            \"speaker\": speaker,\n",
    "            \"value\": text\n",
    "        })\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_words_to_speakers(word_timestamps, diarization_result):\n",
    "    \"\"\"\n",
    "    Allinea i timestamp di ogni parola con gli speaker e raggruppa le parole\n",
    "    in segmenti continui per ogni speaker.\n",
    "    \"\"\"\n",
    "    aligned_segments = []\n",
    "    \n",
    "    # Prepara una lista di turni di speaker per una ricerca più rapida\n",
    "    speaker_turns = []\n",
    "    for turn, _, speaker_label in diarization_result.itertracks(yield_label=True):\n",
    "        speaker_turns.append({\"start\": turn.start, \"end\": turn.end, \"speaker\": speaker_label})\n",
    "\n",
    "    # Variabili per tenere traccia del segmento corrente\n",
    "    current_segment_speaker = None\n",
    "    current_segment_text = \"\"\n",
    "    current_segment_start = 0\n",
    "    \n",
    "    for word_info in word_timestamps:\n",
    "        word_start, word_end = word_info[\"timestamp\"]\n",
    "        word_text = word_info[\"text\"]\n",
    "\n",
    "        # Trova lo speaker per la parola corrente\n",
    "        word_speaker = \"UNKNOWN\"\n",
    "        for turn in speaker_turns:\n",
    "            if turn[\"start\"] <= word_start and turn[\"end\"] >= word_end:\n",
    "                word_speaker = turn[\"speaker\"]\n",
    "                break\n",
    "        \n",
    "        if current_segment_speaker is None:\n",
    "            # Inizia il primo segmento\n",
    "            current_segment_speaker = word_speaker\n",
    "            current_segment_start = word_start\n",
    "\n",
    "        # Se lo speaker cambia, salva il segmento precedente e iniziane uno nuovo\n",
    "        if word_speaker != current_segment_speaker:\n",
    "            if current_segment_text: # Salva solo se c'è del testo\n",
    "                aligned_segments.append({\n",
    "                    \"start_time\": current_segment_start,\n",
    "                    \"stop_time\": last_word_end, # Usa il tempo di fine dell'ultima parola\n",
    "                    \"speaker\": current_segment_speaker,\n",
    "                    \"value\": current_segment_text.strip()\n",
    "                })\n",
    "            \n",
    "            # Inizia un nuovo segmento\n",
    "            current_segment_speaker = word_speaker\n",
    "            current_segment_start = word_start\n",
    "            current_segment_text = \"\"\n",
    "\n",
    "        # Aggiungi la parola al testo del segmento corrente\n",
    "        current_segment_text += word_text\n",
    "        last_word_end = word_end\n",
    "\n",
    "    # Aggiungi l'ultimo segmento rimasto dopo la fine del loop\n",
    "    if current_segment_text:\n",
    "        aligned_segments.append({\n",
    "            \"start_time\": current_segment_start,\n",
    "            \"stop_time\": last_word_end,\n",
    "            \"speaker\": current_segment_speaker,\n",
    "            \"value\": current_segment_text.strip()\n",
    "        })\n",
    "        \n",
    "    return aligned_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-medium.en\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    ").to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=model,\n",
    "  tokenizer=processor.tokenizer,\n",
    "  feature_extractor=processor.feature_extractor,\n",
    "  torch_dtype=torch_dtype,\n",
    "  device=device,\n",
    "  #return_timestamps=True,\n",
    "  return_timestamps=\"word\",\n",
    "  generate_kwargs={\"max_new_tokens\": 400},#, \"language\": \"english\"},\n",
    "  chunk_length_s=30\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "# Inizializza pipeline diarizzazione da HuggingFace\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\")   \n",
    ")\n",
    "diarization_pipeline.to(torch.device(device))\n",
    "\n",
    "sessions = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "\n",
    "# Itera su ogni sessione singolarmente\n",
    "for session in tqdm(sessions, desc=\"Processing sessions\"):\n",
    "    session_path = os.path.join(base_dir, session)\n",
    "    base_name = session.split(\"_\")[0]\n",
    "    audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "    transcript_path = os.path.join(session_path, f\"{base_name}_TRANSCRIPT.csv\")\n",
    "\n",
    "    print(f\"\\nProcessing session: {audio_path}\")\n",
    "\n",
    "    if not os.path.exists(audio_path):\n",
    "        print(f\"Audio file not found for session {session}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Rimuovi la vecchia trascrizione se esiste\n",
    "    old_transcript = os.path.join(session_path, f\"{base_name}_Transcript.csv\")\n",
    "    if os.path.exists(old_transcript):\n",
    "        os.remove(old_transcript)\n",
    "\n",
    "    print(f\"Running diarization...\")\n",
    "    diarization = diarization_pipeline(audio_path)\n",
    "    last_diarization_end = 0\n",
    "    for turn, _, _ in diarization.itertracks(yield_label=True):\n",
    "        if turn.end > last_diarization_end:\n",
    "            last_diarization_end = turn.end\n",
    "\n",
    "    print(f\"\\nTranscribing\")\n",
    "\n",
    "    result = pipe(audio_path)\n",
    "    '''\n",
    "    transcription = result['text']\n",
    "    timestamps = result['chunks']\n",
    "    print(timestamps)\n",
    "    '''\n",
    "    word_timestamps = result['chunks']\n",
    "    print(word_timestamps)\n",
    "    '''\n",
    "    segments = []\n",
    "    for chunk in timestamps:\n",
    "        start_time, end_time = chunk[\"timestamp\"]\n",
    "        if end_time is None:\n",
    "          print(f\"Warning: Found a segment with no end time. Using fallback: {last_diarization_end}\")\n",
    "          end_time = last_diarization_end\n",
    "        segments.append({\n",
    "            \"start\": start_time,\n",
    "            \"end\": end_time,\n",
    "            \"text\": chunk[\"text\"]\n",
    "        })\n",
    "\n",
    "    segments = [\n",
    "      s for s in segments \n",
    "      if s[\"start\"] is not None and s[\"end\"] is not None and s[\"end\"] > s[\"start\"] and s[\"text\"] != \"\"\n",
    "    ]'''\n",
    "\n",
    "    # Se non c'è testo, crea un file vuoto e continua\n",
    "    #if not segments:\n",
    "    if not word_timestamps:\n",
    "        print(f\"No speech detected in {audio_path}. Saving empty transcript.\")\n",
    "        pd.DataFrame(columns=[\"start_time\", \"stop_time\", \"speaker\", \"value\"]).to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "        continue\n",
    "\n",
    "    speakers = list(set(segment[2] for segment in diarization.itertracks(yield_label=True)))\n",
    "    print(f\"Speakers found: {len(speakers)} ({speakers})\")\n",
    "\n",
    "    # 3. Allineamento\n",
    "    #print(f\"Aligning transcription with speakers\")\n",
    "    #aligned = assign_speakers_to_segments(segments, diarization)\n",
    "    print(f\"Aligning transcription with speakers using word-level timestamps...\")\n",
    "    aligned = align_words_to_speakers(word_timestamps, diarization)\n",
    "\n",
    "    df = pd.DataFrame(aligned)\n",
    "    df.to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Saved {transcript_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio_segment(audio_tensor, sample_rate):\n",
    "    # audio_tensor shape: [channels, samples]\n",
    "    audio_np = audio_tensor.numpy().T  # Trasponi per shape (samples, channels)\n",
    "    sd.play(audio_np, sample_rate)\n",
    "    sd.wait()\n",
    "\n",
    "def label_speakers(audio_path, transcript_path):\n",
    "    df = pd.read_csv(transcript_path, sep=\"\\t\")\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    speaker_labels = {}\n",
    "    speakers = df['speaker'].unique()\n",
    "\n",
    "    for spk in speakers:\n",
    "        first_seg = df[df['speaker'] == spk].iloc[0]\n",
    "        start_sample = int(first_seg['start_time'] * sample_rate)\n",
    "        end_sample = int(first_seg['stop_time'] * sample_rate)\n",
    "\n",
    "        segment_audio = waveform[:, start_sample:end_sample]\n",
    "\n",
    "        print(f\"\\nSpeaker: {spk} - playing audio segment from {start_sample/sample_rate:.2f}s to {end_sample/sample_rate:.2f}s\")\n",
    "        play_audio_segment(segment_audio, sample_rate)\n",
    "\n",
    "        choice = input(\"Label this speaker as (E)llie, (P)articipant, (O)ther: \").strip().lower()\n",
    "        if choice == 'e':\n",
    "            speaker_labels[spk] = \"Ellie\"\n",
    "        elif choice == 'p':\n",
    "            speaker_labels[spk] = \"Participant\"\n",
    "        else:\n",
    "            speaker_labels[spk] = \"ignore\"\n",
    "\n",
    "    df['speaker'] = df['speaker'].map(speaker_labels)\n",
    "    df.to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Updated transcript saved to {transcript_path}\")\n",
    "\n",
    "# Process all sessions for speaker labeling\n",
    "sessions = sorted([d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "\n",
    "for session in tqdm(sessions, desc=\"Labeling speakers\"):\n",
    "    session_path = os.path.join(base_dir, session)\n",
    "    base_name = session.split(\"_\")[0]\n",
    "    audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "    transcript_path = os.path.join(session_path, f\"{base_name}_TRANSCRIPT.csv\")\n",
    "    \n",
    "    if os.path.exists(audio_path) and os.path.exists(transcript_path):\n",
    "        print(f\"\\n=== Processing session {session} ===\")\n",
    "        try:\n",
    "            label_speakers(audio_path, transcript_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {session}: {e}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Skipping {session}: missing audio or transcript file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
