{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import whisperx\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MahmoudAshraf97/whisper-diarization/blob/main/Whisper_Transcription_%2B_NeMo_Diarization.ipynb\n",
    "def get_words_speaker_mapping(wrd_ts, spk_ts):\n",
    "    turn_idx = 0\n",
    "    wrd_spk_mapping = []\n",
    "\n",
    "    for wrd_dict in wrd_ts:\n",
    "        ws, we, wrd = wrd_dict[\"start\"], wrd_dict[\"end\"], wrd_dict[\"word\"]\n",
    "\n",
    "        while turn_idx < len(spk_ts) - 1 and ws > spk_ts.iloc[turn_idx][\"end\"]:\n",
    "            turn_idx += 1\n",
    "\n",
    "        current_row = spk_ts.iloc[turn_idx]\n",
    "        wrd_spk_mapping.append({\n",
    "            \"word\": wrd,\n",
    "            \"start_time\": ws,\n",
    "            \"end_time\": we,\n",
    "            \"speaker\": current_row[\"speaker\"]\n",
    "        })\n",
    "\n",
    "    return wrd_spk_mapping\n",
    "\n",
    "sentence_ending_punctuations = \".?!\"\n",
    "\n",
    "def get_realigned_ws_mapping_with_punctuation(word_speaker_mapping, max_words_in_sentence=60):\n",
    "    words_list = [d[\"word\"] for d in word_speaker_mapping]\n",
    "    speaker_list = [d[\"speaker\"] for d in word_speaker_mapping]\n",
    "\n",
    "    # Funzioni di utilità per trovare l'inizio e la fine di una frase\n",
    "    def is_word_sentence_end(idx):\n",
    "        return idx >= 0 and words_list[idx][-1] in sentence_ending_punctuations\n",
    "\n",
    "    def get_first_word_idx(current_idx):\n",
    "        left_idx = current_idx\n",
    "        while (left_idx > 0 and\n",
    "               current_idx - left_idx < max_words_in_sentence and\n",
    "               speaker_list[left_idx - 1] == speaker_list[left_idx] and\n",
    "               not is_word_sentence_end(left_idx - 1)):\n",
    "            left_idx -= 1\n",
    "        return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
    "\n",
    "    def get_last_word_idx(current_idx):\n",
    "        right_idx = current_idx\n",
    "        while (right_idx < len(words_list) - 1 and\n",
    "               right_idx - current_idx < max_words_in_sentence and\n",
    "               not is_word_sentence_end(right_idx)):\n",
    "            right_idx += 1\n",
    "        return right_idx if right_idx == len(words_list) - 1 or is_word_sentence_end(right_idx) else -1\n",
    "\n",
    "    # Itera e corregge\n",
    "    k = 0\n",
    "    while k < len(word_speaker_mapping) - 1:\n",
    "        if speaker_list[k] != speaker_list[k + 1] and not is_word_sentence_end(k):\n",
    "            left_idx = get_first_word_idx(k)\n",
    "            right_idx = get_last_word_idx(k) if left_idx > -1 else -1\n",
    "\n",
    "            if left_idx != -1 and right_idx != -1:\n",
    "                sub_speaker_list = speaker_list[left_idx : right_idx + 1]\n",
    "                # Assegna lo speaker più frequente a tutta la frase\n",
    "                '''dominant_speaker = max(set(sub_speaker_list), key=sub_speaker_list.count)\n",
    "                for i in range(left_idx, right_idx + 1):\n",
    "                    speaker_list[i] = dominant_speaker\n",
    "                k = right_idx'''\n",
    "                dominant_speaker = max(set(sub_speaker_list), key=sub_speaker_list.count)\n",
    "                # Aggiungi un controllo di robustezza\n",
    "                if sub_speaker_list.count(dominant_speaker) >= len(sub_speaker_list) / 2:\n",
    "                    for i in range(left_idx, right_idx + 1):\n",
    "                        speaker_list[i] = dominant_speaker\n",
    "                k = right_idx\n",
    "        k += 1\n",
    "\n",
    "    # Crea la lista riallineata\n",
    "    realigned_list = []\n",
    "    for i, d in enumerate(word_speaker_mapping):\n",
    "        new_dict = d.copy()\n",
    "        new_dict[\"speaker\"] = speaker_list[i]\n",
    "        realigned_list.append(new_dict)\n",
    "\n",
    "    return realigned_list\n",
    "\n",
    "\n",
    "def get_sentences_speaker_mapping(word_speaker_mapping):\n",
    "    \"\"\"\n",
    "    Raggruppa le parole (con speaker assegnato) in frasi.\n",
    "    Una nuova frase inizia quando cambia lo speaker o quando si incontra\n",
    "    una fine di frase grammaticale.\n",
    "    \"\"\"\n",
    "    sentence_checker = nltk.tokenize.PunktSentenceTokenizer().text_contains_sentbreak\n",
    "\n",
    "    sentences = []\n",
    "    current_sentence = None\n",
    "\n",
    "    for wrd_dict in word_speaker_mapping:\n",
    "        word, speaker = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
    "        start_time, end_time = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
    "\n",
    "        is_new_sentence = (\n",
    "            current_sentence is None or\n",
    "            speaker != current_sentence[\"speaker\"] or\n",
    "            sentence_checker(current_sentence[\"text\"] + \" \" + word)\n",
    "        )\n",
    "\n",
    "        if is_new_sentence:\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "            current_sentence = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"text\": word,\n",
    "            }\n",
    "        else:\n",
    "            current_sentence[\"end_time\"] = end_time\n",
    "            current_sentence[\"text\"] += \" \" + word\n",
    "\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70979fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_sessions(dataset_dir, sessions, device, language=\"en\"):\n",
    "    compute_type = \"float16\" if torch.cuda.is_available() else \"float32\"\n",
    "    batch_size=32\n",
    "    model_id = \"tiny.en\"\n",
    "    temp_dir = os.path.join(dataset_dir, \"temp_results\")\n",
    "    os.makedirs(temp_dir, exist_ok=True) \n",
    "\n",
    "    model = whisperx.load_model(model_id, device, compute_type=compute_type, language=language)\n",
    "\n",
    "    for session in tqdm(sessions, desc=\"Trascrizione Audio\"):\n",
    "        session_path = os.path.join(dataset_dir, session)\n",
    "        base_name = session.split(\"_\")[0]\n",
    "        audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "        intermediate_path = os.path.join(temp_dir, f\"{session}_transcript.json\")\n",
    "        if os.path.exists(intermediate_path):\n",
    "            print(f\"Skippo {audio_path} perché la trascrizione esiste già in {intermediate_path}\")\n",
    "            continue\n",
    "        print(f\"\\nSto processando: {audio_path}\")\n",
    "        \n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "        result = model.transcribe(audio, batch_size=batch_size)\n",
    "\n",
    "        with open(intermediate_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Trascrizione intermedia salvata in: {intermediate_path}\")\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf82856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_sessions(dataset_dir, sessions, device, language=\"en\"):\n",
    "    temp_dir = os.path.join(dataset_dir, \"temp_results\")\n",
    "    model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
    "\n",
    "    for session in tqdm(sessions, desc=\"Allineamento Audio\"):\n",
    "        intermediate_path = os.path.join(temp_dir, f\"{session}_transcript.json\")\n",
    "        aligned_path = os.path.join(temp_dir, f\"{session}_aligned.json\")\n",
    "        session_path = os.path.join(dataset_dir, session)\n",
    "        base_name = session.split(\"_\")[0]\n",
    "        audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "        if os.path.exists(aligned_path):\n",
    "            print(f\"Skippo {audio_path} perché l'allineamento esiste già in {aligned_path}\")\n",
    "            continue\n",
    "        print(f\"\\nSto allineando: {session}\")\n",
    "\n",
    "        # Carica il risultato della trascrizione\n",
    "        with open(intermediate_path, 'r', encoding='utf-8') as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "\n",
    "        # Esegui l'allineamento\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        with open(aligned_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Allineamento salvato in: {aligned_path}\")\n",
    "\n",
    "    del model_a\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df92d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_sessions(dataset_dir, sessions, device):\n",
    "    temp_dir = os.path.join(dataset_dir, \"temp_results\")\n",
    "    load_dotenv()\n",
    "    diarize_model = whisperx.diarize.DiarizationPipeline(\n",
    "        use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"), \n",
    "        device=device\n",
    "    )\n",
    "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
    "\n",
    "    for session in tqdm(sessions, desc=\"Diarizzazione Speaker\"):\n",
    "        session_path = os.path.join(dataset_dir, session)\n",
    "        base_name = session.split(\"_\")[0]\n",
    "        audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "        aligned_path = os.path.join(temp_dir, f\"{session}_aligned.json\")\n",
    "        temp_transcript_path = os.path.join(temp_dir, f\"{base_name}_TRANSCRIPT.csv\")\n",
    "        transcript_path = os.path.join(session_path, f\"{base_name}_TRANSCRIPT.csv\")\n",
    "        old_transcript_path = os.path.join(session_path, f\"{base_name}_transcript.csv\")\n",
    "        if os.path.exists(old_transcript_path):\n",
    "            os.remove(old_transcript_path)\n",
    "        \n",
    "        if os.path.exists(temp_transcript_path) or os.path.exists(transcript_path):\n",
    "            print(f\"Skippo {audio_path} perché la trascrizione esiste già in {temp_transcript_path}\")\n",
    "            if not os.path.exists(transcript_path): # per trasferire temp_results fatto in un altra macchina\n",
    "                shutil.copy2(temp_transcript_path, transcript_path)\n",
    "            continue\n",
    "                \n",
    "        print(f\"\\nSto diarizzando: {session}\")\n",
    "        with open(aligned_path, 'r', encoding='utf-8') as f:\n",
    "            aligned_result = json.load(f)\n",
    "        # `whisperx.align` salva i risultati in 'segments' e 'word_segments'.\n",
    "        word_timestamps = aligned_result.get(\"word_segments\")\n",
    "\n",
    "        # Esegui la diarizzazione sull'audio completo\n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "        speaker_timestamps = diarize_model(audio, min_speakers=2, max_speakers=3) \n",
    "\n",
    "        # Assegna gli speaker alle parole\n",
    "        wsm = get_words_speaker_mapping(word_timestamps, speaker_timestamps)\n",
    "        \n",
    "        words_list = [item['word'] for item in wsm]\n",
    "        labeled_words = punct_model.predict(words_list)\n",
    "        # Questa parte serve a migliorare il raggruppamento in frasi\n",
    "        ending_puncts = \".?!\"\n",
    "        model_puncts = \".,;:!?\"\n",
    "        is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
    "        for word_dict, labeled_tuple in zip(wsm, labeled_words):\n",
    "            word = word_dict[\"word\"]\n",
    "            if (word and labeled_tuple[1] in ending_puncts and (word[-1] not in model_puncts or is_acronym(word))):\n",
    "                word += labeled_tuple[1]\n",
    "                if word.endswith(\"..\"): word = word.rstrip(\".\")\n",
    "                word_dict[\"word\"] = word\n",
    "\n",
    "        wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
    "        ssm = get_sentences_speaker_mapping(wsm, speaker_timestamps)\n",
    "\n",
    "        # 6. Salva nel formato CSV finale\n",
    "        final_segments = []\n",
    "        for s in ssm:\n",
    "            final_segments.append({\"start_time\": s[\"start_time\"], \"stop_time\": s[\"end_time\"], \"speaker\": s[\"speaker\"], \"value\": s[\"text\"].strip()})\n",
    "\n",
    "        df = pd.DataFrame(final_segments)\n",
    "        if not df.empty:\n",
    "            df = df[[\"start_time\", \"stop_time\", \"speaker\", \"value\"]]\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=[\"start_time\", \"stop_time\", \"speaker\", \"value\"])\n",
    "\n",
    "        df.to_csv(temp_transcript_path, sep=\"\\t\", index=False)\n",
    "        df.to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    del diarize_model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edaic_dir = \"../datasets/EDAIC-WOZ\"\n",
    "\n",
    "sessions = sorted([d for d in os.listdir(edaic_dir) \\\n",
    "                    if os.path.isdir(os.path.join(edaic_dir, d)) and d.endswith('_P')])\n",
    "transcribe_sessions(edaic_dir, sessions, device)\n",
    "align_sessions(edaic_dir, sessions, device)\n",
    "diarize_sessions(edaic_dir, sessions, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "daic_dir = \"../datasets/DAIC-WOZ\" \n",
    "\n",
    "sessions = [\"318_P\", \"321_P\", \"341_P\", \"362_P\"] # https://github.com/adbailey1/daic_woz_process/tree/master\n",
    "transcribe_sessions(daic_dir, sessions, device)\n",
    "align_sessions(daic_dir, sessions, device)\n",
    "diarize_sessions(daic_dir, sessions, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
