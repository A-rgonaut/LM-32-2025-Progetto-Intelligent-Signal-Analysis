{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bce4fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt install libcudnn8 libcudnn8-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858b18f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import whisperx\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "\n",
    "device =\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b871d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MahmoudAshraf97/whisper-diarization/blob/main/Whisper_Transcription_%2B_NeMo_Diarization.ipynb\n",
    "def get_words_speaker_mapping(wrd_ts, spk_ts):\n",
    "    turn_idx = 0\n",
    "    wrd_spk_mapping = []\n",
    "\n",
    "    for wrd_dict in wrd_ts:\n",
    "        ws, we, wrd = wrd_dict[\"start\"], wrd_dict[\"end\"], wrd_dict[\"word\"]\n",
    "\n",
    "        while turn_idx < len(spk_ts) - 1 and ws > spk_ts.iloc[turn_idx][\"end\"]:\n",
    "            turn_idx += 1\n",
    "\n",
    "        current_row = spk_ts.iloc[turn_idx]\n",
    "        wrd_spk_mapping.append({\n",
    "            \"word\": wrd,\n",
    "            \"start_time\": ws,\n",
    "            \"end_time\": we,\n",
    "            \"speaker\": current_row[\"speaker\"]\n",
    "        })\n",
    "\n",
    "    return wrd_spk_mapping\n",
    "\n",
    "sentence_ending_punctuations = \".?!\"\n",
    "\n",
    "def get_realigned_ws_mapping_with_punctuation(word_speaker_mapping, max_words_in_sentence=60):\n",
    "    words_list = [d[\"word\"] for d in word_speaker_mapping]\n",
    "    speaker_list = [d[\"speaker\"] for d in word_speaker_mapping]\n",
    "\n",
    "    # Funzioni di utilità per trovare l'inizio e la fine di una frase\n",
    "    def is_word_sentence_end(idx):\n",
    "        return idx >= 0 and words_list[idx][-1] in sentence_ending_punctuations\n",
    "\n",
    "    def get_first_word_idx(current_idx):\n",
    "        left_idx = current_idx\n",
    "        while (left_idx > 0 and\n",
    "               current_idx - left_idx < max_words_in_sentence and\n",
    "               speaker_list[left_idx - 1] == speaker_list[left_idx] and\n",
    "               not is_word_sentence_end(left_idx - 1)):\n",
    "            left_idx -= 1\n",
    "        return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
    "\n",
    "    def get_last_word_idx(current_idx):\n",
    "        right_idx = current_idx\n",
    "        while (right_idx < len(words_list) - 1 and\n",
    "               right_idx - current_idx < max_words_in_sentence and\n",
    "               not is_word_sentence_end(right_idx)):\n",
    "            right_idx += 1\n",
    "        return right_idx if right_idx == len(words_list) - 1 or is_word_sentence_end(right_idx) else -1\n",
    "\n",
    "    # Itera e corregge\n",
    "    k = 0\n",
    "    while k < len(word_speaker_mapping) - 1:\n",
    "        if speaker_list[k] != speaker_list[k + 1] and not is_word_sentence_end(k):\n",
    "            left_idx = get_first_word_idx(k)\n",
    "            right_idx = get_last_word_idx(k) if left_idx > -1 else -1\n",
    "\n",
    "            if left_idx != -1 and right_idx != -1:\n",
    "                sub_speaker_list = speaker_list[left_idx : right_idx + 1]\n",
    "                # Assegna lo speaker più frequente a tutta la frase\n",
    "                '''dominant_speaker = max(set(sub_speaker_list), key=sub_speaker_list.count)\n",
    "                for i in range(left_idx, right_idx + 1):\n",
    "                    speaker_list[i] = dominant_speaker\n",
    "                k = right_idx'''\n",
    "                dominant_speaker = max(set(sub_speaker_list), key=sub_speaker_list.count)\n",
    "                # Aggiungi un controllo di robustezza\n",
    "                if sub_speaker_list.count(dominant_speaker) >= len(sub_speaker_list) / 2:\n",
    "                    for i in range(left_idx, right_idx + 1):\n",
    "                        speaker_list[i] = dominant_speaker\n",
    "                k = right_idx\n",
    "        k += 1\n",
    "\n",
    "    # Crea la lista riallineata\n",
    "    realigned_list = []\n",
    "    for i, d in enumerate(word_speaker_mapping):\n",
    "        new_dict = d.copy()\n",
    "        new_dict[\"speaker\"] = speaker_list[i]\n",
    "        realigned_list.append(new_dict)\n",
    "\n",
    "    return realigned_list\n",
    "\n",
    "\n",
    "def get_sentences_speaker_mapping(word_speaker_mapping):\n",
    "    \"\"\"\n",
    "    Raggruppa le parole (con speaker assegnato) in frasi.\n",
    "    Una nuova frase inizia quando cambia lo speaker o quando si incontra\n",
    "    una fine di frase grammaticale.\n",
    "    \"\"\"\n",
    "    sentence_checker = nltk.tokenize.PunktSentenceTokenizer().text_contains_sentbreak\n",
    "\n",
    "    sentences = []\n",
    "    current_sentence = None\n",
    "\n",
    "    for wrd_dict in word_speaker_mapping:\n",
    "        word, speaker = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
    "        start_time, end_time = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
    "\n",
    "        is_new_sentence = (\n",
    "            current_sentence is None or\n",
    "            speaker != current_sentence[\"speaker\"] or\n",
    "            sentence_checker(current_sentence[\"text\"] + \" \" + word)\n",
    "        )\n",
    "\n",
    "        if is_new_sentence:\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "            current_sentence = {\n",
    "                \"speaker\": speaker,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time,\n",
    "                \"text\": word,\n",
    "            }\n",
    "        else:\n",
    "            current_sentence[\"end_time\"] = end_time\n",
    "            current_sentence[\"text\"] += \" \" + word\n",
    "\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70979fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_sessions(dataset_dir, sessions, device, language=\"en\"):\n",
    "    compute_type = \"float16\" if device == 'cuda' else \"float32\"\n",
    "    batch_size=32\n",
    "    model_id = \"large-v2\"\n",
    "    temp_dir = os.path.join(dataset_dir, \"temp_results\")\n",
    "    os.makedirs(temp_dir, exist_ok=True) \n",
    "\n",
    "    model = whisperx.load_model(model_id, device, compute_type=compute_type, language=language,local_files_only=False)\n",
    "\n",
    "    for session in tqdm(sessions, desc=\"Trascrizione Audio\"):\n",
    "        session_path = os.path.join(dataset_dir, session)\n",
    "        base_name = session.split(\"_\")[0]\n",
    "        audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "        intermediate_path = os.path.join(temp_dir, f\"{session}_transcript.json\")\n",
    "        if os.path.exists(intermediate_path):\n",
    "            print(f\"Skippo {audio_path} perché la trascrizione esiste già in {intermediate_path}\")\n",
    "            continue\n",
    "        print(f\"\\nSto processando: {audio_path}\")\n",
    "        \n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "        result = model.transcribe(audio, batch_size=batch_size)\n",
    "\n",
    "        with open(intermediate_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Trascrizione intermedia salvata in: {intermediate_path}\")\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf82856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_sessions(dataset_dir, sessions, device, language=\"en\"):\n",
    "    temp_dir = os.path.join(dataset_dir, \"temp_results\")\n",
    "    model_a, metadata = whisperx.load_align_model(language_code=language, device=device)\n",
    "\n",
    "    for session in tqdm(sessions, desc=\"Allineamento Audio\"):\n",
    "        intermediate_path = os.path.join(temp_dir, f\"{session}_transcript.json\")\n",
    "        aligned_path = os.path.join(temp_dir, f\"{session}_aligned.json\")\n",
    "        if os.path.exists(aligned_path):\n",
    "            print(f\"Skippo {audio_path} perché l'allineamento esiste già in {aligned_path}\")\n",
    "            continue\n",
    "        print(f\"\\nSto allineando: {session}\")\n",
    "\n",
    "        # Carica il risultato della trascrizione\n",
    "        with open(intermediate_path, 'r', encoding='utf-8') as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        # Per l'allineamento è necessario ricaricare l'audio\n",
    "        session_path = os.path.join(dataset_dir, session)\n",
    "        base_name = session.split(\"_\")[0]\n",
    "        audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "\n",
    "        # Esegui l'allineamento\n",
    "        result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "        with open(aligned_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Allineamento salvato in: {aligned_path}\")\n",
    "\n",
    "    del model_a\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df92d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_sessions(dataset_dir, sessions, device):\n",
    "    temp_dir = os.path.join(dataset_dir, \"temp_results\")\n",
    "    load_dotenv()\n",
    "    diarize_model = whisperx.diarize.DiarizationPipeline(\n",
    "        use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"), \n",
    "        device=device\n",
    "    )\n",
    "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
    "\n",
    "    for session in tqdm(sessions, desc=\"Diarizzazione Speaker\"):\n",
    "        session_path = os.path.join(dataset_dir, session)\n",
    "        base_name = session.split(\"_\")[0]\n",
    "        audio_path = os.path.join(session_path, f\"{base_name}_AUDIO.wav\")\n",
    "        aligned_path = os.path.join(temp_dir, f\"{session}_aligned.json\")\n",
    "        temp_transcript_path = os.path.join(temp_dir, f\"{base_name}_TRANSCRIPT.csv\")\n",
    "        transcript_path = os.path.join(session_path, f\"{base_name}_TRANSCRIPT.csv\")\n",
    "        old_transcript_path = os.path.join(session_path, f\"{base_name}_transcript.csv\")\n",
    "        if os.path.exists(old_transcript_path):\n",
    "            os.remove(old_transcript_path)\n",
    "        \n",
    "        if os.path.exists(temp_transcript_path) or os.path.exists(transcript_path):\n",
    "            print(f\"Skippo {audio_path} perché la trascrizione esiste già in {temp_transcript_path}\")\n",
    "            if not os.path.exists(transcript_path): # per trasferire temp_results fatto in un altra macchina\n",
    "                shutil.copy2(temp_transcript_path, transcript_path)\n",
    "            continue\n",
    "                \n",
    "        print(f\"\\nSto diarizzando: {session}\")\n",
    "        with open(aligned_path, 'r', encoding='utf-8') as f:\n",
    "            aligned_result = json.load(f)\n",
    "        # `whisperx.align` salva i risultati in 'segments' e 'word_segments'.\n",
    "        word_timestamps = aligned_result.get(\"word_segments\")\n",
    "\n",
    "        # Esegui la diarizzazione sull'audio completo\n",
    "        audio = whisperx.load_audio(audio_path)\n",
    "        speaker_timestamps = diarize_model(audio, min_speakers=2, max_speakers=3) \n",
    "\n",
    "        # Assegna gli speaker alle parole\n",
    "        wsm = get_words_speaker_mapping(word_timestamps, speaker_timestamps)\n",
    "        \n",
    "        words_list = [item['word'] for item in wsm]\n",
    "        labeled_words = punct_model.predict(words_list)\n",
    "        # Questa parte serve a migliorare il raggruppamento in frasi\n",
    "        ending_puncts = \".?!\"\n",
    "        model_puncts = \".,;:!?\"\n",
    "        is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
    "        for word_dict, labeled_tuple in zip(wsm, labeled_words):\n",
    "            word = word_dict[\"word\"]\n",
    "            if (word and labeled_tuple[1] in ending_puncts and (word[-1] not in model_puncts or is_acronym(word))):\n",
    "                word += labeled_tuple[1]\n",
    "                if word.endswith(\"..\"): word = word.rstrip(\".\")\n",
    "                word_dict[\"word\"] = word\n",
    "\n",
    "        wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
    "        ssm = get_sentences_speaker_mapping(wsm)\n",
    "\n",
    "        # 6. Salva nel formato CSV finale\n",
    "        final_segments = []\n",
    "        for s in ssm:\n",
    "            final_segments.append({\"start_time\": s[\"start_time\"], \"stop_time\": s[\"end_time\"], \"speaker\": s[\"speaker\"], \"value\": s[\"text\"].strip()})\n",
    "\n",
    "        df = pd.DataFrame(final_segments)\n",
    "        if not df.empty:\n",
    "            df = df[[\"start_time\", \"stop_time\", \"speaker\", \"value\"]]\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=[\"start_time\", \"stop_time\", \"speaker\", \"value\"])\n",
    "\n",
    "        df.to_csv(temp_transcript_path, sep=\"\\t\", index=False)\n",
    "        df.to_csv(transcript_path, sep=\"\\t\", index=False)\n",
    "\n",
    "    del diarize_model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f87d00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trascrizione Audio: 100%|██████████| 29/29 [00:00<00:00, 28981.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skippo ../datasets/EDAIC-WOZ\\602_P\\602_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\602_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\604_P\\604_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\604_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\617_P\\617_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\617_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\624_P\\624_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\624_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\633_P\\633_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\633_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\636_P\\636_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\636_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\637_P\\637_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\637_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\638_P\\638_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\638_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\640_P\\640_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\640_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\641_P\\641_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\641_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\649_P\\649_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\649_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\655_P\\655_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\655_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\658_P\\658_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\658_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\659_P\\659_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\659_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\661_P\\661_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\661_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\673_P\\673_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\673_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\677_P\\677_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\677_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\680_P\\680_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\680_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\682_P\\682_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\682_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\684_P\\684_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\684_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\688_P\\688_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\688_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\689_P\\689_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\689_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\691_P\\691_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\691_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\696_P\\696_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\696_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\698_P\\698_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\698_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\699_P\\699_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\699_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\705_P\\705_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\705_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\709_P\\709_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\709_P_transcript.json\n",
      "Skippo ../datasets/EDAIC-WOZ\\716_P\\716_AUDIO.wav perché la trascrizione esiste già in ../datasets/EDAIC-WOZ\\temp_results\\716_P_transcript.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Device set to use cuda:0\n",
      "c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:181: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n",
      "Diarizzazione Speaker:   0%|          | 0/29 [00:00<?, ?it/s]c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\site-packages\\pyannote\\audio\\utils\\reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 602_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "Diarizzazione Speaker:   3%|▎         | 1/29 [00:15<07:06, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 604_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:   7%|▋         | 2/29 [00:26<05:55, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 617_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Diarizzazione Speaker:  10%|█         | 3/29 [00:53<08:18, 19.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 624_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  14%|█▍        | 4/29 [01:09<07:30, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 633_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  17%|█▋        | 5/29 [01:40<09:07, 22.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 636_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  21%|██        | 6/29 [02:13<10:03, 26.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 637_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  24%|██▍       | 7/29 [02:45<10:19, 28.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 638_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  28%|██▊       | 8/29 [02:55<07:49, 22.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 640_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  31%|███       | 9/29 [03:11<06:44, 20.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 641_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  34%|███▍      | 10/29 [03:25<05:48, 18.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 649_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  38%|███▊      | 11/29 [03:44<05:33, 18.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 655_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  41%|████▏     | 12/29 [04:00<05:02, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 658_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  45%|████▍     | 13/29 [04:19<04:49, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 659_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  48%|████▊     | 14/29 [04:47<05:19, 21.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 661_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  52%|█████▏    | 15/29 [05:12<05:11, 22.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 673_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  55%|█████▌    | 16/29 [05:23<04:06, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 677_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  59%|█████▊    | 17/29 [05:41<03:41, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 680_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  62%|██████▏   | 18/29 [06:08<03:51, 21.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 682_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  66%|██████▌   | 19/29 [06:21<03:08, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 684_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  69%|██████▉   | 20/29 [06:42<02:53, 19.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 688_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  72%|███████▏  | 21/29 [07:03<02:39, 19.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 689_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  76%|███████▌  | 22/29 [07:20<02:13, 19.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 691_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  79%|███████▉  | 23/29 [07:45<02:04, 20.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 696_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  83%|████████▎ | 24/29 [07:59<01:34, 18.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 698_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  86%|████████▌ | 25/29 [08:26<01:25, 21.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 699_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  90%|████████▉ | 26/29 [08:39<00:56, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 705_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  93%|█████████▎| 27/29 [08:54<00:35, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 709_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  97%|█████████▋| 28/29 [09:10<00:17, 17.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 716_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker: 100%|██████████| 29/29 [09:28<00:00, 19.62s/it]\n"
     ]
    }
   ],
   "source": [
    "edaic_dir = \"../datasets/EDAIC-WOZ\"\n",
    "\n",
    "sessions = sorted([d for d in os.listdir(edaic_dir) \\\n",
    "                    if os.path.isdir(os.path.join(edaic_dir, d)) and d.endswith('_P')])\n",
    "transcribe_sessions(edaic_dir, sessions, device)\n",
    "#align_sessions(edaic_dir, sessions, device)\n",
    "diarize_sessions(edaic_dir, sessions, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ca9625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trascrizione Audio:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto processando: ../datasets/DAIC-WOZ\\318_P\\318_AUDIO.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trascrizione Audio:  25%|██▌       | 1/4 [02:21<07:05, 141.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trascrizione intermedia salvata in: ../datasets/DAIC-WOZ\\temp_results\\318_P_transcript.json\n",
      "\n",
      "Sto processando: ../datasets/DAIC-WOZ\\321_P\\321_AUDIO.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trascrizione Audio:  50%|█████     | 2/4 [05:52<06:04, 182.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trascrizione intermedia salvata in: ../datasets/DAIC-WOZ\\temp_results\\321_P_transcript.json\n",
      "\n",
      "Sto processando: ../datasets/DAIC-WOZ\\341_P\\341_AUDIO.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trascrizione Audio:  75%|███████▌  | 3/4 [09:49<03:27, 207.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trascrizione intermedia salvata in: ../datasets/DAIC-WOZ\\temp_results\\341_P_transcript.json\n",
      "\n",
      "Sto processando: ../datasets/DAIC-WOZ\\362_P\\362_AUDIO.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trascrizione Audio: 100%|██████████| 4/4 [12:20<00:00, 185.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trascrizione intermedia salvata in: ../datasets/DAIC-WOZ\\temp_results\\362_P_transcript.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Allineamento Audio:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto allineando: 318_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Allineamento Audio:  25%|██▌       | 1/4 [00:07<00:22,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allineamento salvato in: ../datasets/DAIC-WOZ\\temp_results\\318_P_aligned.json\n",
      "\n",
      "Sto allineando: 321_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Allineamento Audio:  50%|█████     | 2/4 [00:16<00:16,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allineamento salvato in: ../datasets/DAIC-WOZ\\temp_results\\321_P_aligned.json\n",
      "\n",
      "Sto allineando: 341_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Allineamento Audio:  75%|███████▌  | 3/4 [00:25<00:08,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allineamento salvato in: ../datasets/DAIC-WOZ\\temp_results\\341_P_aligned.json\n",
      "\n",
      "Sto allineando: 362_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Allineamento Audio: 100%|██████████| 4/4 [00:31<00:00,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allineamento salvato in: ../datasets/DAIC-WOZ\\temp_results\\362_P_aligned.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Device set to use cuda:0\n",
      "c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:181: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"none\"` instead.\n",
      "  warnings.warn(\n",
      "Diarizzazione Speaker:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 318_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anto-\\Miniconda3\\envs\\speech_project\\lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "Diarizzazione Speaker:  25%|██▌       | 1/4 [00:11<00:33, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 321_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  50%|█████     | 2/4 [00:26<00:27, 13.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 341_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker:  75%|███████▌  | 3/4 [00:43<00:14, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sto diarizzando: 362_P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Diarizzazione Speaker: 100%|██████████| 4/4 [00:54<00:00, 13.57s/it]\n"
     ]
    }
   ],
   "source": [
    "daic_dir = \"../datasets/DAIC-WOZ\" \n",
    "\n",
    "sessions = [\"318_P\", \"321_P\", \"341_P\", \"362_P\"] # https://github.com/adbailey1/daic_woz_process/tree/master\n",
    "transcribe_sessions(daic_dir, sessions, device)\n",
    "align_sessions(daic_dir, sessions, device)\n",
    "diarize_sessions(daic_dir, sessions, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d4548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
