{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from tqdm import tqdm\n",
    "from utils import load_labels_from_dataset, get_audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc12a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementazione dell'architettura CNN+MLP descritta nel paper.\n",
    "    La rete accetta in input segmenti di waveform audio (1D).\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(CNNMLP, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=64, stride=1),\n",
    "            nn.BatchNorm1d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=dropout_rate)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=32, stride=1),\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=dropout_rate)\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=dropout_rate)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mlp_block = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=128), # Placeholder, verrà inizializzato dinamicamente\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features=128, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self._mlp_initialized = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x_flattened = self.flatten(x)\n",
    "        if not self._mlp_initialized:\n",
    "            in_features = x_flattened.shape[1]\n",
    "            self.mlp_block[0] = nn.Linear(in_features, 128).to(x.device)\n",
    "            print(f\"MLP inizializzato dinamicamente con {in_features} feature di input.\")\n",
    "            self._mlp_initialized = True\n",
    "        output = self.mlp_block(x_flattened)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672a57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSegmentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset PyTorch generico per caricare e segmentare file audio.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths, labels, sr=16000, segment_ms=250, hop_ms=50):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.sr = sr\n",
    "        self.segment_length = int(sr * (segment_ms / 1000.0))\n",
    "        self.hop_length = int(sr * (hop_ms / 1000.0))\n",
    "        self.segments = []\n",
    "        self.segment_labels = []\n",
    "        self._create_segments()\n",
    "\n",
    "    def _create_segments(self):\n",
    "        print(\"Creazione dei segmenti dal dataset...\")\n",
    "        for i, file_path in enumerate(self.file_paths):\n",
    "            label = self.labels[i]\n",
    "            try:\n",
    "                waveform, original_sr = librosa.load(file_path, sr=None)\n",
    "                if np.max(np.abs(waveform)) > 0:\n",
    "                    waveform = waveform / np.max(np.abs(waveform))\n",
    "                start = 0\n",
    "                while start + self.segment_length <= len(waveform):\n",
    "                    segment = waveform[start : start + self.segment_length]\n",
    "                    self.segments.append(segment)\n",
    "                    self.segment_labels.append(label)\n",
    "                    start += self.hop_length\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante l'elaborazione del file {file_path}: {e}\")\n",
    "        print(f\"Creati {len(self.segments)} segmenti totali.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        segment = self.segments[idx]\n",
    "        label = self.segment_labels[idx]\n",
    "        segment_tensor = torch.tensor(segment, dtype=torch.float32).unsqueeze(0)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        return segment_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e798c6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo in uso: cpu\n",
      "\n",
      "File per training: 107, Etichette: 107\n",
      "File per validation: 35, Etichette: 35\n",
      "File per test: 47, Etichette: 47\n",
      "Creazione dei segmenti dal dataset...\n",
      "Creati 1050247 segmenti totali.\n",
      "\n",
      "--- Inizio Addestramento ---\n"
     ]
    }
   ],
   "source": [
    "# --- Impostazioni e Iperparametri ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo in uso: {DEVICE}\")\n",
    "\n",
    "dataset_name = \"datasets/DAIC-WOZ-Cleaned\"\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "DROPOUT_RATE = 0.5\n",
    "SR = 16000\n",
    "SEGMENT_MS = 250\n",
    "HOP_MS = 50\n",
    "\n",
    "# --- Caricamento dei DataFrame ---\n",
    "try:\n",
    "    train_df = pd.read_csv(os.path.join(dataset_name, 'train_split_Depression_AVEC2017.csv'))\n",
    "    dev_df = pd.read_csv(os.path.join(dataset_name, 'dev_split_Depression_AVEC2017.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(dataset_name, 'full_test_split.csv'))\n",
    "\n",
    "    # --- Estrazione di Percorsi e Etichette ---\n",
    "    y_train = load_labels_from_dataset(train_df)\n",
    "    y_dev = load_labels_from_dataset(dev_df) \n",
    "    y_test = load_labels_from_dataset(test_df)\n",
    "\n",
    "    train_paths = get_audio_paths(train_df, dataset_name)\n",
    "    dev_paths = get_audio_paths(dev_df, dataset_name)\n",
    "    test_paths = get_audio_paths(test_df, dataset_name)\n",
    "\n",
    "    print(f\"\\nFile per training: {len(train_paths)}, Etichette: {len(y_train)}\")\n",
    "    print(f\"File per validation: {len(dev_paths)}, Etichette: {len(y_dev)}\")\n",
    "    print(f\"File per test: {len(test_paths)}, Etichette: {len(y_test)}\")\n",
    "\n",
    "    # --- Inizializzazione di Dataset e DataLoader ---\n",
    "    train_dataset = AudioSegmentDataset(\n",
    "        train_paths, y_train, sr=SR, segment_ms=SEGMENT_MS, hop_ms=HOP_MS\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    # --- Inizializzazione Modello, Loss e Ottimizzatore ---\n",
    "    model = CNNMLP(dropout_rate=DROPOUT_RATE).to(DEVICE)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # --- Ciclo di Addestramento e Valutazione ---\n",
    "    print(\"\\n--- Inizio Addestramento ---\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        avg_train_loss = train_loop(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        \n",
    "        # Valutazione sul set di sviluppo (dev) alla fine di ogni epoca\n",
    "        acc, sens, spec = evaluate(model, dev_paths, y_dev, DEVICE, sr=SR, segment_ms=SEGMENT_MS, hop_ms=HOP_MS)\n",
    "        \n",
    "        print(f\"\\nEpoca {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        print(f\"  Loss di Addestramento: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Metriche di Sviluppo (Dev) -> Accuratezza: {acc:.4f} | Sensitività: {sens:.4f} | Specificità: {spec:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Addestramento Completato ---\")\n",
    "\n",
    "    # --- Valutazione Finale sul Test Set ---\n",
    "    print(\"\\n--- Valutazione Finale sul Test Set ---\")\n",
    "    test_acc, test_sens, test_spec = evaluate(model, test_paths, y_test, DEVICE, sr=SR, segment_ms=SEGMENT_MS, hop_ms=HOP_MS)\n",
    "    print(f\"Accuratezza Test: {test_acc:.4f}\")\n",
    "    print(f\"Sensitività Test (Recall classe 1): {test_sens:.4f}\")\n",
    "    print(f\"Specificità Test (Recall classe 0): {test_spec:.4f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERRORE: Uno dei file CSV non è stato trovato nella cartella '{dataset_name}'.\")\n",
    "    print(\"Assicurati che il percorso sia corretto e che i file 'train_split_Depression_AVEC2017.csv',\")\n",
    "    print(\"'dev_split_Depression_AVEC2017.csv' e 'full_test_split.csv' esistano.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
