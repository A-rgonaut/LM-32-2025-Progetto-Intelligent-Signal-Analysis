{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad73d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from utils import load_labels_from_dataset, get_audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parametri di Configurazione dal Paper ---\n",
    "\n",
    "# Parametri audio e di segmentazione (Pag. 8, colonna sx)\n",
    "SR = 16000\n",
    "SEGMENT_MS = 250  # Durata del segmento in ms\n",
    "HOP_MS = 50       # Spostamento (shift) tra segmenti in ms\n",
    "\n",
    "# Parametri di training (Tabella IV)\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 100          # Il paper usa 100 iterazioni, che interpretiamo come epoche\n",
    "DROPOUT_RATE = 0.25\n",
    "\n",
    "# Parametri di Early Stopping (Pag. 8, menzionato nel testo)\n",
    "EARLY_STOPPING_PATIENCE = 5 # \"five epochs with no improvement\"\n",
    "\n",
    "# Altri parametri\n",
    "MODEL_SAVE_PATH = \"parkinson_detection_cnn_best_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilizzo del dispositivo: {DEVICE}\")\n",
    "\n",
    "# Impostazione del seed per la riproducibilitÃ \n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementazione dell'architettura CNN+MLP descritta nella Tabella III del paper.\n",
    "    La rete accetta in input segmenti di waveform audio (1D).\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.25, num_classes=2):\n",
    "        super(CNNMLP, self).__init__()\n",
    "        \n",
    "        # Blocco Convoluzionale 1\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=64, stride=1),\n",
    "            nn.BatchNorm1d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Blocco Convoluzionale 2\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=32, stride=1),\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Blocco Convoluzionale 3\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=16, stride=1),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Il blocco MLP viene inizializzato dinamicamente nel forward pass\n",
    "        # per adattarsi alla dimensione dell'output delle CNN.\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mlp_block = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features=1, out_features=128), # Inizializzato dinamicamente\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "        self._mlp_initialized = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        \n",
    "        x_flattened = self.flatten(x)\n",
    "        \n",
    "        # Inizializzazione dinamica del primo layer MLP\n",
    "        if not self._mlp_initialized:\n",
    "            in_features = x_flattened.shape[1]\n",
    "            self.mlp_block[1] = nn.Linear(in_features, 128).to(x.device)\n",
    "            print(f\"MLP inizializzato dinamicamente con {in_features} feature di input.\")\n",
    "            self._mlp_initialized = True\n",
    "            \n",
    "        output = self.mlp_block(x_flattened)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioSegmentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset PyTorch per caricare file audio e suddividerli in segmenti\n",
    "    secondo le specifiche del paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths, labels, sr, segment_ms, hop_ms):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.sr = sr\n",
    "        self.segment_length = int(sr * (segment_ms / 1000.0))\n",
    "        self.hop_length = int(sr * (hop_ms / 1000.0))\n",
    "        \n",
    "        self.segments = []\n",
    "        self.segment_labels = []\n",
    "        \n",
    "        print(\"Creazione dei segmenti dal dataset...\")\n",
    "        for i, file_path in enumerate(tqdm(self.file_paths, desc=\"Segmentazione Audio\")):\n",
    "            label = self.labels[i]\n",
    "            try:\n",
    "                waveform, _ = librosa.load(file_path, sr=self.sr)\n",
    "                \n",
    "                # Normalizzazione per evitare problemi con audio a basso volume\n",
    "                if np.max(np.abs(waveform)) > 0:\n",
    "                    waveform = waveform / np.max(np.abs(waveform))\n",
    "                \n",
    "                start = 0\n",
    "                while start + self.segment_length <= len(waveform):\n",
    "                    segment = waveform[start : start + self.segment_length]\n",
    "                    self.segments.append(segment)\n",
    "                    self.segment_labels.append(label)\n",
    "                    start += self.hop_length\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante l'elaborazione del file {file_path}: {e}\")\n",
    "        \n",
    "        print(f\"Creati {len(self.segments)} segmenti totali.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        segment = self.segments[idx]\n",
    "        label = self.segment_labels[idx]\n",
    "        # Aggiunge la dimensione del canale (1) richiesta da Conv1d\n",
    "        segment_tensor = torch.tensor(segment, dtype=torch.float32).unsqueeze(0)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return segment_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7aee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Attiva l'early stopping se la metrica di validazione non migliora.\"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = -np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if current_score > self.best_score + self.min_delta:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    \"\"\"Esegue un'epoca di training.\"\"\"\n",
    "    model.train()\n",
    "    total_loss, correct_predictions = 0, 0\n",
    "    \n",
    "    for batch_segments, batch_labels in tqdm(data_loader, desc=\"Training Epoch\", leave=False):\n",
    "        batch_segments = batch_segments.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_segments)\n",
    "        loss = loss_fn(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == batch_labels)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
    "    return avg_loss, accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, file_paths, labels, device, sr, segment_ms, hop_ms):\n",
    "    \"\"\"\n",
    "    Valuta il modello su file audio completi, mediando le predizioni dei segmenti,\n",
    "    come descritto nel paper. Ritorna accuratezza, sensitivitÃ  e specificitÃ .\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    segment_length = int(sr * (segment_ms / 1000.0))\n",
    "    hop_length = int(sr * (hop_ms / 1000.0))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for file_path, label in tqdm(zip(file_paths, labels), desc=\"Valutazione\", total=len(file_paths), leave=False):\n",
    "            try:\n",
    "                waveform, _ = librosa.load(file_path, sr=sr)\n",
    "                if np.max(np.abs(waveform)) > 0:\n",
    "                    waveform = waveform / np.max(np.abs(waveform))\n",
    "                \n",
    "                segments = []\n",
    "                start = 0\n",
    "                while start + segment_length <= len(waveform):\n",
    "                    segment = waveform[start : start + segment_length]\n",
    "                    segments.append(segment)\n",
    "                    start += hop_length\n",
    "                \n",
    "                if not segments: continue\n",
    "                \n",
    "                segments_tensor = torch.tensor(np.array(segments), dtype=torch.float32).unsqueeze(1).to(device)\n",
    "                \n",
    "                segment_outputs = model(segments_tensor)\n",
    "                segment_probs = torch.softmax(segment_outputs, dim=1)\n",
    "                \n",
    "                avg_probs = torch.mean(segment_probs, dim=0)\n",
    "                final_prediction = torch.argmax(avg_probs).item()\n",
    "                \n",
    "                all_predictions.append(final_prediction)\n",
    "                all_targets.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante la valutazione del file {file_path}: {e}\")\n",
    "\n",
    "    if not all_targets:\n",
    "        return 0.0, 0.0, 0.0, np.array([])\n",
    "    \n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    \n",
    "    if len(cm.ravel()) == 4:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else: # Gestisce il caso di predizioni tutte di una classe\n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "        if len(np.unique(all_targets)) == 1: # Se ci sono solo etichette 0 o 1\n",
    "             if np.unique(all_targets)[0] == 0: tn = cm[0,0]\n",
    "             else: tp = cm[0,0]\n",
    "        else: # Se ci sono entrambe le etichette ma la matrice Ã¨ 1x1\n",
    "             tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    \n",
    "    return accuracy, sensitivity, specificity, cm, all_targets, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"datasets/DAIC-WOZ-Cleaned\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(dataset_name, 'train_split_Depression_AVEC2017.csv'))\n",
    "dev_df = pd.read_csv(os.path.join(dataset_name, 'dev_split_Depression_AVEC2017.csv'))\n",
    "test_df = pd.read_csv(os.path.join(dataset_name, 'full_test_split.csv'))\n",
    "\n",
    "y_train = load_labels_from_dataset(train_df)\n",
    "y_dev = load_labels_from_dataset(dev_df) \n",
    "y_test = load_labels_from_dataset(test_df)\n",
    "\n",
    "train_paths = get_audio_paths(train_df, dataset_name)\n",
    "dev_paths = get_audio_paths(dev_df, dataset_name)\n",
    "test_paths = get_audio_paths(test_df, dataset_name)\n",
    "\n",
    "print(f\"File di training: {len(train_paths)}, di validazione: {len(dev_paths)}, di test: {len(test_paths)}\")\n",
    "\n",
    "# Creazione Dataset e DataLoader\n",
    "train_dataset = AudioSegmentDataset(train_paths, y_train, sr=SR, segment_ms=SEGMENT_MS, hop_ms=HOP_MS)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Inizializzazione Modello, Ottimizzatore e Loss\n",
    "model = CNNMLP(dropout_rate=DROPOUT_RATE, num_classes=2).to(DEVICE)\n",
    "# L'ottimizzatore nel paper Ã¨ SGD\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopper = EarlyStopping(patience=EARLY_STOPPING_PATIENCE)\n",
    "\n",
    "print(f\"\\nModello inizializzato con {sum(p.numel() for p in model.parameters() if p.requires_grad)} parametri allenabili.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ciclo di Training\n",
    "print(\"\\n=== Inizio Training ===\")\n",
    "best_val_accuracy = -1\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "    print(f\"Training -> Loss: {train_loss:.4f}, Accuracy (sui segmenti): {train_acc:.4f}\")\n",
    "\n",
    "    # Validazione\n",
    "    val_acc, val_sens, val_spec, _, _, _ = evaluate_model(model, dev_paths, y_dev, DEVICE, SR, SEGMENT_MS, HOP_MS)\n",
    "    print(f\"Validation -> Accuracy: {val_acc:.4f}, Sensitivity: {val_sens:.4f}, Specificity: {val_spec:.4f}\")\n",
    "    \n",
    "    # Salvataggio del miglior modello basato sull'accuratezza di validazione\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Nuova migliore accuratezza: {best_val_accuracy:.4f}. Modello salvato in '{MODEL_SAVE_PATH}'.\")\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopper(val_acc)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"Early stopping attivato.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n=== Training Completato ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da03fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Valutazione Finale sul Test Set ===\")\n",
    "\n",
    "# Carica il miglior modello salvato\n",
    "best_model = CNNMLP(dropout_rate=DROPOUT_RATE, num_classes=2).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "print(f\"Miglior modello caricato da '{MODEL_SAVE_PATH}'.\")\n",
    "\n",
    "# Valutazione\n",
    "test_acc, test_sens, test_spec, test_cm, test_targets, test_preds = evaluate_model(\n",
    "    best_model, test_paths, y_test, DEVICE, SR, SEGMENT_MS, HOP_MS\n",
    ")\n",
    "\n",
    "print(\"\\n--- Risultati sul Test Set ---\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Sensitivity: {test_sens:.4f}\")\n",
    "print(f\"Specificity: {test_spec:.4f}\")\n",
    "\n",
    "print(\"\\nMatrice di Confusione:\")\n",
    "print(test_cm)\n",
    "\n",
    "print(\"\\nReport di Classificazione Dettagliato:\")\n",
    "print(classification_report(test_targets, test_preds, target_names=['Non-Depressed (0)', 'Depressed (1)']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
