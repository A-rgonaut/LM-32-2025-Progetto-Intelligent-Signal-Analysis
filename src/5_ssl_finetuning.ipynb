{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "529d4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils import load_labels_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2074e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDepressionDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, model_name, sample_rate=16_000, segment_length_seconds=20, max_segments=None):\n",
    "        self.audio_paths = audio_paths  \n",
    "        self.labels = labels            \n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name, do_normalize=False)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length_seconds = segment_length_seconds\n",
    "        self.segment_length_samples = segment_length_seconds * sample_rate\n",
    "        self.max_segments = max_segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def _load_audio(self, audio_path):\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = audio.mean(axis=0)\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        return audio\n",
    "    \n",
    "    def _segment_audio(self, audio):\n",
    "        \"\"\"Segmenta l'audio in chunks di lunghezza fissa\"\"\"\n",
    "        segments = []\n",
    "        \n",
    "        # Se l'audio è più corto del segmento desiderato, pad con zeri\n",
    "        if len(audio) < self.segment_length_samples:\n",
    "            padded_audio = np.zeros(self.segment_length_samples)\n",
    "            padded_audio[:len(audio)] = audio\n",
    "            segments.append(padded_audio)\n",
    "        else:\n",
    "            # Dividi in segmenti\n",
    "            for i in range(0, len(audio), self.segment_length_samples):\n",
    "                segment = audio[i:i + self.segment_length_samples]\n",
    "                \n",
    "                # Se l'ultimo segmento è troppo corto, pad con zeri\n",
    "                if len(segment) < self.segment_length_samples:\n",
    "                    padded_segment = np.zeros(self.segment_length_samples)\n",
    "                    padded_segment[:len(segment)] = segment\n",
    "                    segment = padded_segment\n",
    "                \n",
    "                segments.append(segment)\n",
    "                \n",
    "                # Limita il numero di segmenti se specificato\n",
    "                if self.max_segments and len(segments) >= self.max_segments:\n",
    "                    break\n",
    "        \n",
    "        return np.array(segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        audio = self._load_audio(audio_path)\n",
    "        segments = self._segment_audio(audio)\n",
    "        \n",
    "        segment_features = []\n",
    "        for segment in segments:\n",
    "            features = self.feature_extractor(\n",
    "                segment, \n",
    "                sampling_rate=self.sample_rate,\n",
    "                max_length=self.segment_length_samples,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                return_attention_mask=False,\n",
    "            )\n",
    "            segment_features.append(features.input_values[0])\n",
    "        \n",
    "        segment_features = torch.stack(segment_features)  # (num_segments, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'input_values': segment_features, \n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'num_segments': len(segments)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73290837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveStatisticsPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Attentive Statistics Pooling based on\n",
    "    \"Attentive Statistics Pooling for Deep Speaker Embedding\" (https://www.isca-archive.org/interspeech_2018/okabe18_interspeech.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, attention_dim=64):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, attention_dim)\n",
    "        # BatchNorm1d is applied on the channel dimension\n",
    "        self.bn = nn.BatchNorm1d(attention_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, seq_len, input_dim).\n",
    "        Returns:\n",
    "            The output tensor of shape (batch_size, input_dim * 2).\n",
    "        \"\"\"\n",
    "        # (batch_size, seq_len, input_dim) -> (batch_size, seq_len, attention_dim)\n",
    "        x_attn = self.linear1(x)\n",
    "\n",
    "        # BatchNorm requires shape (batch_size, channels, seq_len), so we transpose\n",
    "        # (batch_size, seq_len, attention_dim) -> (batch_size, attention_dim, seq_len)\n",
    "        x_attn = x_attn.transpose(1, 2)\n",
    "        x_attn = self.bn(x_attn)\n",
    "        # Transpose back to the original dimension order\n",
    "        # (batch_size, attention_dim, seq_len) -> (batch_size, seq_len, attention_dim)\n",
    "        x_attn = x_attn.transpose(1, 2)\n",
    "\n",
    "        # Apply activation and final linear layer\n",
    "        x_attn = self.relu(x_attn)\n",
    "        # (batch_size, seq_len, attention_dim) -> (batch_size, seq_len, 1)\n",
    "        attention_scores = self.linear2(x_attn)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Equation (5): Weighted mean\n",
    "        # (batch_size, seq_len, 1) * (batch_size, seq_len, input_dim) -> (batch_size, input_dim)\n",
    "        mean = torch.sum(attention_weights * x, dim=1)\n",
    "\n",
    "        # Equation (6): Weighted standard deviation\n",
    "        # E[X^2] - (E[X])^2\n",
    "        # (batch_size, seq_len, 1) * (batch_size, seq_len, input_dim) -> (batch_size, input_dim)\n",
    "        variance = torch.sum(attention_weights * x.pow(2), dim=1) - mean.pow(2)\n",
    "        std_dev = torch.sqrt(variance.clamp(min=1e-6))\n",
    "\n",
    "        # (batch_size, input_dim), (batch_size, input_dim) -> (batch_size, input_dim * 2)\n",
    "        pooled_output = torch.cat((mean, std_dev), dim=1)\n",
    "\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c89dacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressionClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout=0.1, \n",
    "                 sequence_model_type='bilstm', sequence_hidden_size=256):\n",
    "        super(DepressionClassifier, self).__init__()\n",
    "    \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        self.sequence_model_type = sequence_model_type\n",
    "\n",
    "        self.ssl_model = AutoModel.from_pretrained(self.model_name, output_hidden_states=True)\n",
    "        self.ssl_hidden_size = self.ssl_model.config.hidden_size\n",
    "        self.head_hidden_size = self.ssl_hidden_size\n",
    "        \n",
    "        # +1 perchè prendiamo anche il layer che fa feature extraction\n",
    "        layers_to_aggregate = self.ssl_model.config.num_hidden_layers + 1\n",
    "        self.layer_weights = nn.Parameter(torch.ones(layers_to_aggregate))\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(self.ssl_hidden_size) for _ in range(layers_to_aggregate)\n",
    "        ])\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Segment-level pooling\n",
    "        self.segment_pooling = AttentiveStatisticsPooling(input_dim=self.ssl_hidden_size)\n",
    "        segment_embedding_dim = self.ssl_hidden_size * 2\n",
    "\n",
    "        # Sequence model per aggregare i segmenti\n",
    "        if sequence_model_type == 'bilstm':\n",
    "            self.sequence_model = nn.LSTM(\n",
    "                input_size=segment_embedding_dim,\n",
    "                hidden_size=sequence_hidden_size,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            sequence_output_dim = sequence_hidden_size * 2  # bidirectional\n",
    "        elif sequence_model_type == 'transformer':\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=segment_embedding_dim,\n",
    "                nhead=8,\n",
    "                dim_feedforward=sequence_hidden_size,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.sequence_model = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "            sequence_output_dim = segment_embedding_dim\n",
    "\n",
    "        # Global pooling\n",
    "        self.global_pooling = AttentiveStatisticsPooling(input_dim=sequence_output_dim)\n",
    "        global_embedding_dim = sequence_output_dim * 2\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(global_embedding_dim, self.head_hidden_size),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.head_hidden_size, self.num_classes),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # initialize weights of classifier\n",
    "        for name, param in self.classifier.named_parameters():\n",
    "            if 'weight' in name and len(param.shape) > 1:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_values = batch['input_values']  # (batch_size, num_segments, seq_len)\n",
    "        batch_size, num_segments, seq_len = input_values.shape\n",
    "        \n",
    "        # IMPORTANTE: Processa ogni audio separatamente per mantenere la struttura gerarchica\n",
    "        all_segment_embeddings = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            # Prendi tutti i segmenti di un singolo audio\n",
    "            single_audio_segments = input_values[batch_idx]  # (num_segments, seq_len)\n",
    "            \n",
    "            # Processa tutti i segmenti di questo audio insieme\n",
    "            ssl_hidden_states = self.ssl_model(\n",
    "                input_values=single_audio_segments,\n",
    "                return_dict=True,\n",
    "            ).hidden_states\n",
    "            \n",
    "            # Weighted aggregation of layers\n",
    "            ssl_hidden_state = torch.zeros_like(ssl_hidden_states[-1])\n",
    "            weights = self.softmax(self.layer_weights)\n",
    "            for i in range(len(ssl_hidden_states)):\n",
    "                ssl_hidden_state += weights[i] * self.layer_norms[i](ssl_hidden_states[i])\n",
    "            \n",
    "            # Attention pooling per ogni segmento di questo audio\n",
    "            segment_embeddings = []\n",
    "            for seg_idx in range(num_segments):\n",
    "                segment_emb = self.segment_pooling(ssl_hidden_state[seg_idx])  # (hidden_size * 2)\n",
    "                segment_embeddings.append(segment_emb)\n",
    "            \n",
    "            segment_embeddings = torch.stack(segment_embeddings)  # (num_segments, hidden_size * 2)\n",
    "            all_segment_embeddings.append(segment_embeddings)\n",
    "        \n",
    "        # Stack embeddings di tutti gli audio nel batch\n",
    "        all_segment_embeddings = torch.stack(all_segment_embeddings)  # (batch_size, num_segments, hidden_size * 2)\n",
    "        \n",
    "        # Sequence modeling per ogni audio nel batch\n",
    "        if self.sequence_model_type == 'bilstm':\n",
    "            sequence_output, _ = self.sequence_model(all_segment_embeddings)\n",
    "        elif self.sequence_model_type == 'transformer':\n",
    "            sequence_output = self.sequence_model(all_segment_embeddings)\n",
    "        \n",
    "        # Global pooling per ogni audio\n",
    "        global_embeddings = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            global_emb = self.global_pooling(sequence_output[batch_idx])\n",
    "            global_embeddings.append(global_emb)\n",
    "        \n",
    "        global_embeddings = torch.stack(global_embeddings)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.classifier(global_embeddings)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efceb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di collate per gestire batch con numero variabile di segmenti\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Questa funzione serve perché diversi audio possono avere numero diverso di segmenti.\n",
    "    Ad esempio:\n",
    "    - Audio 1: 30 secondi → 3 segmenti da 10s\n",
    "    - Audio 2: 50 secondi → 5 segmenti da 10s\n",
    "    \n",
    "    Per creare un batch uniforme, dobbiamo fare padding al numero massimo di segmenti.\n",
    "    Viene chiamata automaticamente dal DataLoader quando batch_size > 1.\n",
    "    \"\"\"\n",
    "    # Trova il numero massimo di segmenti nel batch\n",
    "    max_segments = max([item['num_segments'] for item in batch])\n",
    "    \n",
    "    batch_input_values = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for item in batch:\n",
    "        input_values = item['input_values']\n",
    "        num_segments = item['num_segments']\n",
    "        \n",
    "        # Pad se necessario (aggiunge segmenti di zeri)\n",
    "        if num_segments < max_segments:\n",
    "            padding_shape = (max_segments - num_segments, input_values.shape[1])\n",
    "            padding = torch.zeros(padding_shape, dtype=input_values.dtype)\n",
    "            input_values = torch.cat([input_values, padding], dim=0)\n",
    "        \n",
    "        batch_input_values.append(input_values)\n",
    "        batch_labels.append(item['label'])\n",
    "    \n",
    "    return {\n",
    "        'input_values': torch.stack(batch_input_values),\n",
    "        'label': torch.stack(batch_labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c04f601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidebonura/miniconda3/lib/python3.12/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Summary ===\n",
      "Total parameters: 100,513,937\n",
      "Trainable parameters: 100,513,937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/davidebonura/miniconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/Users/davidebonura/miniconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/davidebonura/miniconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "  File \"/Users/davidebonura/miniconda3/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "                ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^^^AttributeError^^: \n",
      "Can't get attribute 'AudioDepressionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "AttributeError: Can't get attribute 'AudioDepressionDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 97582, 97583) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/multiprocessing/connection.py:1135\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 97583) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    106\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 108\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Sposta i dati su GPU\u001b[39;49;00m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 97582, 97583) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Parametri\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "num_classes = 2  # Binary classification (depressed vs non-depressed)\n",
    "dataset_name = \"datasets/DAIC-WOZ-Cleaned\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'train_split_Depression_AVEC2017.csv'))\n",
    "dev_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'dev_split_Depression_AVEC2017.csv'))\n",
    "test_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'full_test_split.csv'))\n",
    "\n",
    "y_train = load_labels_from_dataset(train_df)\n",
    "y_dev = load_labels_from_dataset(dev_df) \n",
    "y_test = load_labels_from_dataset(test_df)\n",
    "\n",
    "def get_audio_paths(df, dataset_name):\n",
    "    audio_paths = []\n",
    "    for participant_id in df['Participant_ID']:\n",
    "        dir_name = f\"{participant_id}_P\"\n",
    "        wav_path = os.path.join(dataset_name, dir_name, f\"{participant_id}_AUDIO.wav\")\n",
    "        if os.path.isfile(wav_path):\n",
    "            audio_paths.append(wav_path)\n",
    "        else:\n",
    "            print(f\"Warning: File non trovato per {participant_id} in {wav_path}\")\n",
    "    return audio_paths\n",
    "\n",
    "# Carica i path audio\n",
    "train_paths = get_audio_paths(train_df, dataset_name)\n",
    "dev_paths = get_audio_paths(dev_df, dataset_name)\n",
    "test_paths = get_audio_paths(test_df, dataset_name)\n",
    "\n",
    "# Crea i dataset\n",
    "train_dataset = AudioDepressionDataset(\n",
    "    audio_paths=train_paths,\n",
    "    labels=y_train,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=10,  # Segmenti da 10 secondi\n",
    "    max_segments=140  # Massimo 140 segmenti (= 23.3 minuti max)\n",
    ")\n",
    "\n",
    "dev_dataset = AudioDepressionDataset(\n",
    "    audio_paths=dev_paths,\n",
    "    labels=y_dev,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=10,\n",
    "    max_segments=140\n",
    ")\n",
    "\n",
    "test_dataset = AudioDepressionDataset(\n",
    "    audio_paths=test_paths,\n",
    "    labels=y_test,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=10,\n",
    "    max_segments=140\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=8,  # Riduci se hai problemi di memoria\n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2   \n",
    ")\n",
    "\n",
    "# Modello\n",
    "model = DepressionClassifier(\n",
    "    model_name=model_name,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1,\n",
    "    sequence_model_type='bilstm',  # Prova anche 'transformer'\n",
    "    sequence_hidden_size=256\n",
    ")\n",
    "\n",
    "# Ottimizzatore e loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        # Sposta i dati su GPU\n",
    "        batch['input_values'] = batch['input_values'].to(device)\n",
    "        batch['label'] = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch['label'])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch} completed. Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            batch['input_values'] = batch['input_values'].to(device)\n",
    "            batch['label'] = batch['label'].to(device)\n",
    "            \n",
    "            output = model(batch)\n",
    "            val_loss += criterion(output, batch['label']).item()\n",
    "            \n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            total += batch['label'].size(0)\n",
    "            correct += (predictions == batch['label']).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(dev_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
