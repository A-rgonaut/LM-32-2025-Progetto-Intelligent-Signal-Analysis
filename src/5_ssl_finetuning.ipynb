{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529d4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoFeatureExtractor, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score \n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import load_labels_from_dataset, get_audio_paths\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value) # Se stai usando la GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Controlla se la GPU è disponibile\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2074e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDepressionDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, model_name, sample_rate=16_000, segment_length_seconds=20, max_segments=None):\n",
    "        self.audio_paths = audio_paths  \n",
    "        self.labels = labels            \n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name, do_normalize=False)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length_seconds = segment_length_seconds\n",
    "        self.segment_length_samples = segment_length_seconds * sample_rate\n",
    "        self.max_segments = max_segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def _load_audio(self, audio_path):\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = audio.mean(axis=0)\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        return audio\n",
    "    \n",
    "    def _segment_audio(self, audio):\n",
    "        \"\"\"Segmenta l'audio in chunks di lunghezza fissa\"\"\"\n",
    "        segments = []\n",
    "        \n",
    "        # Se l'audio è più corto del segmento desiderato, pad con zeri\n",
    "        if len(audio) < self.segment_length_samples:\n",
    "            padded_audio = np.zeros(self.segment_length_samples)\n",
    "            padded_audio[:len(audio)] = audio\n",
    "            segments.append(padded_audio)\n",
    "        else:\n",
    "            # Dividi in segmenti\n",
    "            for i in range(0, len(audio), self.segment_length_samples):\n",
    "                segment = audio[i:i + self.segment_length_samples]\n",
    "                \n",
    "                # Se l'ultimo segmento è troppo corto, pad con zeri\n",
    "                if len(segment) < self.segment_length_samples:\n",
    "                    padded_segment = np.zeros(self.segment_length_samples)\n",
    "                    padded_segment[:len(segment)] = segment\n",
    "                    segment = padded_segment\n",
    "                \n",
    "                segments.append(segment)\n",
    "                \n",
    "                # Limita il numero di segmenti se specificato\n",
    "                if self.max_segments and len(segments) >= self.max_segments:\n",
    "                    break\n",
    "        \n",
    "        return np.array(segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        audio = self._load_audio(audio_path)\n",
    "        segments = self._segment_audio(audio)\n",
    "        \n",
    "        segment_features = []\n",
    "        for segment in segments:\n",
    "            features = self.feature_extractor(\n",
    "                segment, \n",
    "                sampling_rate=self.sample_rate,\n",
    "                max_length=self.segment_length_samples,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "            segment_features.append(features.input_values[0])\n",
    "        \n",
    "        segment_features = torch.stack(segment_features)  # (num_segments, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'input_values': segment_features, \n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'num_segments': len(segments)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73290837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPoolingLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, seq_len, embed_dim).\n",
    "            mask: The padding mask of shape (batch_size, seq_len).\n",
    "        Returns:\n",
    "            The output tensor of shape (batch_size, embed_dim).\n",
    "        \"\"\"\n",
    "        weights = self.linear(x)  # (bs, seq_len, embed_dim) -> (bs, seq_len, 1)\n",
    "\n",
    "        # Apply the mask before softmax to ignore padding\n",
    "        if mask is not None:\n",
    "            # .unsqueeze(-1): (bs, seq_len) -> (bs, seq_len, 1)\n",
    "            # Assign a very negative value where the mask is True (padding)\n",
    "            weights.masked_fill_(mask.unsqueeze(-1), -1e9)\n",
    "\n",
    "        weights = torch.softmax(weights, dim=1)  # Now masked elements will have ~0 weight\n",
    "\n",
    "        # Weighted sum (bs, seq_len, 1) * (bs, seq_len, embed_dim) -> (bs, embed_dim)\n",
    "        x = torch.sum(weights * x, dim=1) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89dacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressionClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout=0.1, \n",
    "                 seq_model_type='bilstm', seq_hidden_size=256):\n",
    "        super(DepressionClassifier, self).__init__()\n",
    "    \n",
    "        # SSL model loading & config\n",
    "        self.ssl_model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.ssl_hidden_size = self.ssl_model.config.hidden_size # e.g. 768\n",
    "\n",
    "        # Weighted sum of SSL model's hidden layers\n",
    "        num_ssl_layers = self.ssl_model.config.num_hidden_layers\n",
    "        layers_to_aggregate = num_ssl_layers + 1 # +1 for the initial embeddings\n",
    "\n",
    "        self.layer_weights = nn.Parameter(torch.ones(layers_to_aggregate))\n",
    "        self.layer_norms = nn.ModuleList(\n",
    "            [nn.LayerNorm(self.ssl_hidden_size) for _ in range(layers_to_aggregate)]\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Segment-level pooling\n",
    "        self.segment_embeddings_pooling = AttentionPoolingLayer(embed_dim=self.ssl_hidden_size)\n",
    "        self.segment_embedding_dim = self.ssl_hidden_size \n",
    "\n",
    "        self.seq_model_type = seq_model_type\n",
    "\n",
    "        if self.seq_model_type == 'bilstm':\n",
    "            self.sequence_model = nn.LSTM(\n",
    "                input_size=self.ssl_hidden_size,\n",
    "                hidden_size=seq_hidden_size,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            self.seq_output_dim = seq_hidden_size * 2  # bidirectional\n",
    "        elif self.seq_model_type == 'transformer':\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=self.ssl_hidden_size,\n",
    "                nhead=8,  # nhead must be a divisor of ssl_hidden_size (e.g. 768 % 8 == 0)\n",
    "                dim_feedforward=seq_hidden_size * 2, # Common practice\n",
    "                dropout=dropout,\n",
    "                activation='relu',\n",
    "                batch_first=True \n",
    "            )\n",
    "            self.sequence_model = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "            self.seq_output_dim = self.ssl_hidden_size\n",
    "\n",
    "        self.audio_embedding_pooling = AttentionPoolingLayer(embed_dim=self.seq_output_dim)\n",
    "        self.audio_embedding_dim = self.seq_output_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.audio_embedding_dim, self.ssl_hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.ssl_hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # initialize weights of classifier\n",
    "        for name, param in self.classifier.named_parameters():\n",
    "            if 'weight' in name and len(param.shape) > 1:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_values = batch['input_values']\n",
    "        attention_mask = batch.get('attention_mask', None) \n",
    "        batch_size, num_segments, seq_len = input_values.shape\n",
    "        \n",
    "        # Reshape from (bs, num_segments, seq_len) to (bs * num_segments, seq_len)\n",
    "        # This allows processing all segments from all audio files in one go.\n",
    "        input_values_flat = input_values.view(batch_size * num_segments, seq_len)\n",
    "        \n",
    "        ssl_hidden_states = self.ssl_model(\n",
    "            input_values=input_values_flat,\n",
    "            return_dict=True,\n",
    "        ).hidden_states  # (bs * num_segments, num_frames, hidden_size)\n",
    "\n",
    "        # Combine all hidden layers from the SSL model using learned weights.\n",
    "        ssl_hidden_state = torch.zeros_like(ssl_hidden_states[-1])  # (bs * num_segments, num_frames, hidden_size)\n",
    "        weights = self.softmax(self.layer_weights)\n",
    "        for i in range(len(ssl_hidden_states)):\n",
    "            ssl_hidden_state += weights[i] * self.layer_norms[i](ssl_hidden_states[i])\n",
    "\n",
    "        # Pool the sequence of frames into a single representation for the whole segment.\n",
    "        segment_embeddings_flat = self.segment_embeddings_pooling(ssl_hidden_state)  # (bs * num_segments, segment_embedding_dim)\n",
    "\n",
    "        # Un-flatten the batch to restore sequence structure \n",
    "        # Reshape from (bs * num_segments, segment_embedding_dim) back to (bs, num_segments, segment_embedding_dim)\n",
    "        segment_embeddings = segment_embeddings_flat.view(batch_size, num_segments, self.segment_embedding_dim)\n",
    "\n",
    "        # Sequence modeling across segments\n",
    "        # Process the sequence of segment embeddings for each audio file.\n",
    "        if self.seq_model_type == 'bilstm':\n",
    "            sequence_output, _ = self.sequence_model(segment_embeddings)\n",
    "        elif self.seq_model_type == 'transformer':\n",
    "            sequence_output = self.sequence_model(segment_embeddings, src_key_padding_mask=attention_mask)\n",
    "        # Result shape: (bs, num_segments, seq_output_dim)\n",
    "        \n",
    "        # Pool the sequence of segments into a single representation for the whole audio file.\n",
    "        audio_embeddings = self.audio_embedding_pooling(sequence_output, mask=attention_mask)  # (bs, audio_embedding_dim)\n",
    "\n",
    "        output = self.classifier(audio_embeddings)  # (bs, num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efceb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Questa funzione serve perché diversi audio possono avere numero diverso di segmenti.\n",
    "    Ad esempio:\n",
    "    - Audio 1: 30 secondi → 3 segmenti da 10s\n",
    "    - Audio 2: 50 secondi → 5 segmenti da 10s\n",
    "    \n",
    "    Per creare un batch uniforme, dobbiamo fare padding al numero massimo di segmenti.\n",
    "    Viene chiamata automaticamente dal DataLoader quando batch_size > 1.\n",
    "    \"\"\"\n",
    "    # Trova il numero massimo di segmenti nel batch\n",
    "    max_segments = max([item['num_segments'] for item in batch])\n",
    "    \n",
    "    batch_input_values = []\n",
    "    batch_labels = []\n",
    "    batch_masks = []\n",
    "    \n",
    "    for item in batch:\n",
    "        input_values = item['input_values']\n",
    "        num_segments = item['num_segments']\n",
    "\n",
    "        mask = torch.zeros(max_segments, dtype=torch.bool)\n",
    "        mask[num_segments:] = True\n",
    "        batch_masks.append(mask)\n",
    "\n",
    "        # Pad se necessario (aggiunge segmenti di zeri)\n",
    "        if num_segments < max_segments:\n",
    "            padding_shape = (max_segments - num_segments, input_values.shape[1])\n",
    "            padding = torch.zeros(padding_shape, dtype=input_values.dtype)\n",
    "            input_values = torch.cat([input_values, padding], dim=0)\n",
    "        \n",
    "        batch_input_values.append(input_values)\n",
    "        batch_labels.append(item['label'])\n",
    "\n",
    "    return {\n",
    "        'input_values': torch.stack(batch_input_values),\n",
    "        'label': torch.stack(batch_labels),\n",
    "        'attention_mask': torch.stack(batch_masks)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c04f601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\configuration_utils.py:381: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-base\"\n",
    "num_classes = 2 \n",
    "dataset_name = \"datasets/DAIC-WOZ-Cleaned\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(dataset_name, 'train_split_Depression_AVEC2017.csv'))\n",
    "dev_df = pd.read_csv(os.path.join(dataset_name, 'dev_split_Depression_AVEC2017.csv'))\n",
    "test_df = pd.read_csv(os.path.join(dataset_name, 'full_test_split.csv'))\n",
    "\n",
    "y_train = load_labels_from_dataset(train_df)\n",
    "y_dev = load_labels_from_dataset(dev_df) \n",
    "y_test = load_labels_from_dataset(test_df)\n",
    "\n",
    "train_paths = get_audio_paths(train_df, dataset_name)\n",
    "dev_paths = get_audio_paths(dev_df, dataset_name)\n",
    "test_paths = get_audio_paths(test_df, dataset_name)\n",
    "\n",
    "# Datasets\n",
    "segment_length_seconds = 4  # Segmenti da 10 secondi\n",
    "max_segments = 20  # Massimo 138 segmenti (23 minuti max)\n",
    "\n",
    "train_dataset = AudioDepressionDataset(\n",
    "    audio_paths=train_paths,\n",
    "    labels=y_train,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=segment_length_seconds,\n",
    "    max_segments=max_segments\n",
    ")\n",
    "\n",
    "dev_dataset = AudioDepressionDataset(\n",
    "    audio_paths=dev_paths,\n",
    "    labels=y_dev,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=segment_length_seconds,\n",
    "    max_segments=max_segments   \n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Modello\n",
    "model = DepressionClassifier(\n",
    "    model_name=model_name,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1,\n",
    "    seq_model_type='bilstm',  # Prova anche 'transformer'\n",
    "    seq_hidden_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b91bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\"\"\"\n",
    "def print_model_summary(model):\n",
    "    print(\"-\"*89)\n",
    "    print(f\"| Layer Name                                              | # of Parameters | Trainable |\")\n",
    "    print(\"-\"*89)\n",
    "    total_num_trainable_params = 0\n",
    "    for layer_name, layer_params in model.named_parameters():\n",
    "        if layer_params.requires_grad:\n",
    "            total_num_trainable_params += layer_params.numel()\n",
    "        print(f\"| {layer_name:<55} | {layer_params.numel():<15} | {str(layer_params.requires_grad):<9} |\")\n",
    "    print(\"-\"*89)\n",
    "    print(f\"| Total # of Parameters: {total_num_trainable_params:<62} |\" )\n",
    "    print(\"-\"*89)\n",
    "\n",
    "#print_model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7df8a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione delle classi nel training set:\n",
      "0    77\n",
      "1    30\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_series = pd.Series(y_train)\n",
    "\n",
    "# Conta il numero di campioni per ciascuna classe\n",
    "class_distribution = labels_series.value_counts().sort_index()\n",
    "print(\"Distribuzione delle classi nel training set:\")\n",
    "print(class_distribution)\n",
    "\n",
    "label_names = ['Non Depresso', 'Depresso']\n",
    "counts = [class_distribution.get(i, 0) for i in range(len(label_names))]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(label_names, counts, color=['tab:red', 'tab:blue', 'tab:green', 'tab:orange'])\n",
    "plt.xlabel(\"Classi\")\n",
    "plt.ylabel(\"Numero di campioni\")\n",
    "plt.title(\"Distribuzione delle classi nel training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a83f7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = -np.inf if mode == 'max' else np.inf\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.mode == 'max':\n",
    "            improvement = (current_score - self.best_score) > self.min_delta\n",
    "        else:\n",
    "            improvement = (self.best_score - current_score) > self.min_delta\n",
    "\n",
    "        if improvement:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True  # Early stop\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7cc541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di addestramento\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, device,epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct_predictions = 0, 0\n",
    "    train_pbar = tqdm(enumerate(train_dataloader), \n",
    "                      total=len(train_dataloader),\n",
    "                      desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "    for batch_idx, batch in train_pbar:\n",
    "        # Preleva i dati dal batch\n",
    "        batch['input_values'] = batch['input_values'].to(device)\n",
    "        batch['label'] = batch['label'].to(device)\n",
    "        if 'attention_mask' in batch:\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calcolo dei logits grezzi\n",
    "        outputs = model(batch)\n",
    "\n",
    "        # Calcolo della loss (i logits vengono passati così come sono)\n",
    "        loss = loss_fn(outputs, batch['label'])\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calcolo delle predizioni: per multi-classe usiamo argmax sui logits\n",
    "        softmax = nn.LogSoftmax(dim=1)\n",
    "        preds = softmax(outputs).argmax(dim=1)\n",
    "        correct_predictions += torch.sum(preds == batch['label'])\n",
    "\n",
    "        # Backpropagation e aggiornamento dei pesi\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Aggiorna lo scheduler se necessario\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / len(data_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Funzione per validazione\n",
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss, correct_predictions = 0, 0\n",
    "    predictions, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch['input_values'] = batch['input_values'].to(device)\n",
    "            batch['label'] = batch['label'].to(device)\n",
    "            if 'attention_mask' in batch:\n",
    "                batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(batch)\n",
    "            loss = loss_fn(outputs,  batch['label'])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs,dim=1)\n",
    "            correct_predictions += torch.sum(preds == batch['label'])\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            targets.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(targets, predictions, average='macro')\n",
    "    return total_loss / len(data_loader), correct_predictions.double() / len(data_loader.dataset), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb6c7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training:   0%|          | 0/54 [00:00<?, ?it/s]C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/10 - Training: 100%|██████████| 54/54 [04:24<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6242, Train Accuracy: 0.7196\n",
      "Validation Loss: 0.6445, Validation Accuracy: 0.6571, Validation F1: 0.3966\n",
      "Nuovo miglior F1: 0.3966\n",
      "\n",
      "=== Epoch 2/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Training:   0%|          | 0/54 [00:00<?, ?it/s]C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 2/10 - Training: 100%|██████████| 54/54 [04:19<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6062, Train Accuracy: 0.7196\n",
      "Validation Loss: 0.6382, Validation Accuracy: 0.6571, Validation F1: 0.3966\n",
      "\n",
      "=== Epoch 3/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Training:   0%|          | 0/54 [00:00<?, ?it/s]C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 3/10 - Training:  28%|██▊       | 15/54 [02:14<05:50,  8.99s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader, loss_fn, optimizer, scheduler, device, epoch, num_epochs)\u001b[0m\n\u001b[0;32m      5\u001b[0m train_pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dataloader), \n\u001b[0;32m      6\u001b[0m                   total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader),\n\u001b[0;32m      7\u001b[0m                   desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m train_pbar:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Preleva i dati dal batch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ottimizzatore e loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.005, mode='max')\n",
    "model_save_path = \"depression_classifier_best.pth\"\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Scheduler per il learning rate\n",
    "num_epochs = 10\n",
    "total_steps  = len(train_dataloader) * num_epochs\n",
    "num_warmup_steps = total_steps // 10\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "best_val_f1 = -np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n=== Epoch {epoch + 1}/{num_epochs} ===\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        device,\n",
    "        epoch,\n",
    "        num_epochs\n",
    "    )\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc, val_f1 = eval_model(\n",
    "        model, \n",
    "        dev_dataloader, \n",
    "        criterion, \n",
    "        device\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "    # Salvataggio miglior modello\n",
    "    if val_f1 > best_val_f1 + early_stopping.min_delta:\n",
    "        best_val_f1 = val_f1 \n",
    "        best_model_weights = model.state_dict().copy()\n",
    "        print(f\"Nuovo miglior F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    if early_stopping(val_f1):\n",
    "        print(f\"Early stopping attivato. Miglior F1: {best_val_f1:.4f}\")\n",
    "        break\n",
    "print(\"Training Completato\")\n",
    "print(f\"Miglior F1 Score: {best_val_f1:.4f}\")\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaf247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Training loop\\nnum_epochs = 5\\nmodel.to(device)\\n\\nearly_stopping = EarlyStopping(patience=3, min_delta=0.01, mode=\\'max\\')\\nbest_val_accuracy = 0.0\\nmodel_save_path = \"depression_classifier_best.pth\"\\n\\nfor epoch in range(num_epochs):\\n    model.train()\\n    total_loss = 0\\n    train_pbar = tqdm(enumerate(train_dataloader), \\n                      total=len(train_dataloader),\\n                      desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\\n\\n    print(f\"\\nEpoch {epoch}\")\\n    \\n    for batch_idx, batch in train_pbar:\\n        batch[\\'input_values\\'] = batch[\\'input_values\\'].to(device)\\n        batch[\\'label\\'] = batch[\\'label\\'].to(device)\\n        if \\'attention_mask\\' in batch:\\n            batch[\\'attention_mask\\'] = batch[\\'attention_mask\\'].to(device)\\n\\n        optimizer.zero_grad()\\n        output = model(batch)\\n        loss = criterion(output, batch[\\'label\\'])\\n\\n        loss.backward()\\n        optimizer.step()\\n\\n        total_loss += loss.item()\\n\\n    avg_loss = total_loss / len(train_dataloader)\\n    print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\\n\\n    \\n    # Validation\\n    model.eval()\\n    val_loss = 0\\n    correct = 0\\n    total = 0\\n    \\n    with torch.no_grad():\\n        for batch in dev_dataloader:\\n            batch[\\'input_values\\'] = batch[\\'input_values\\'].to(device)\\n            batch[\\'label\\'] = batch[\\'label\\'].to(device)\\n            if \\'attention_mask\\' in batch:\\n                batch[\\'attention_mask\\'] = batch[\\'attention_mask\\'].to(device)\\n            \\n            output = model(batch)\\n            val_loss += criterion(output, batch[\\'label\\']).item()\\n            \\n            predictions = torch.argmax(output, dim=1)\\n            total += batch[\\'label\\'].size(0)\\n            correct += (predictions == batch[\\'label\\']).sum().item()\\n    \\n    val_accuracy = correct / total\\n    avg_val_loss = val_loss / len(dev_dataloader)\\n    print(f\\'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}\\')\\n    if val_accuracy > best_val_accuracy:\\n        best_val_accuracy = val_accuracy\\n        torch.save(model.state_dict(), model_save_path)\\n        print(f\"New best model saved to {model_save_path} with accuracy: {val_accuracy:.4f}\")'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Training loop\n",
    "num_epochs = 5\n",
    "model.to(device)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.01, mode='max')\n",
    "best_val_accuracy = 0.0\n",
    "model_save_path = \"depression_classifier_best.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_pbar = tqdm(enumerate(train_dataloader), \n",
    "                      total=len(train_dataloader),\n",
    "                      desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in train_pbar:\n",
    "        batch['input_values'] = batch['input_values'].to(device)\n",
    "        batch['label'] = batch['label'].to(device)\n",
    "        if 'attention_mask' in batch:\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch['label'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            batch['input_values'] = batch['input_values'].to(device)\n",
    "            batch['label'] = batch['label'].to(device)\n",
    "            if 'attention_mask' in batch:\n",
    "                batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            \n",
    "            output = model(batch)\n",
    "            val_loss += criterion(output, batch['label']).item()\n",
    "            \n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            total += batch['label'].size(0)\n",
    "            correct += (predictions == batch['label']).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(dev_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved to {model_save_path} with accuracy: {val_accuracy:.4f}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e083f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\anto-\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\configuration_utils.py:381: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Results ===\n",
      "Test Loss: 0.6051, Test Accuracy: 0.7021, Test F1: 0.4125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = AudioDepressionDataset(\n",
    "    audio_paths=test_paths,\n",
    "    labels=y_test,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=segment_length_seconds,\n",
    "    max_segments=max_segments\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Carica il miglior modello\n",
    "#model.load_state_dict(best_model_weights)\n",
    "model.eval()\n",
    "test_loss, test_acc, test_f1 = eval_model(\n",
    "    model, \n",
    "    test_dataloader, \n",
    "    criterion, \n",
    "    device\n",
    ")\n",
    "print(f\"\\n=== Test Results ===\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Test F1: {test_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
