{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import load_labels_from_dataset, get_audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDepressionDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, model_name, sample_rate=16_000, segment_length_seconds=20, max_segments=None):\n",
    "        self.audio_paths = audio_paths  \n",
    "        self.labels = labels            \n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name, do_normalize=False)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length_seconds = segment_length_seconds\n",
    "        self.segment_length_samples = segment_length_seconds * sample_rate\n",
    "        self.max_segments = max_segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def _load_audio(self, audio_path):\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = audio.mean(axis=0)\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        return audio\n",
    "    \n",
    "    def _segment_audio(self, audio):\n",
    "        \"\"\"Segmenta l'audio in chunks di lunghezza fissa\"\"\"\n",
    "        segments = []\n",
    "        \n",
    "        # Se l'audio è più corto del segmento desiderato, pad con zeri\n",
    "        if len(audio) < self.segment_length_samples:\n",
    "            padded_audio = np.zeros(self.segment_length_samples)\n",
    "            padded_audio[:len(audio)] = audio\n",
    "            segments.append(padded_audio)\n",
    "        else:\n",
    "            # Dividi in segmenti\n",
    "            for i in range(0, len(audio), self.segment_length_samples):\n",
    "                segment = audio[i:i + self.segment_length_samples]\n",
    "                \n",
    "                # Se l'ultimo segmento è troppo corto, pad con zeri\n",
    "                if len(segment) < self.segment_length_samples:\n",
    "                    padded_segment = np.zeros(self.segment_length_samples)\n",
    "                    padded_segment[:len(segment)] = segment\n",
    "                    segment = padded_segment\n",
    "                \n",
    "                segments.append(segment)\n",
    "                \n",
    "                # Limita il numero di segmenti se specificato\n",
    "                if self.max_segments and len(segments) >= self.max_segments:\n",
    "                    break\n",
    "        \n",
    "        return np.array(segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        audio = self._load_audio(audio_path)\n",
    "        segments = self._segment_audio(audio)\n",
    "        \n",
    "        segment_features = []\n",
    "        for segment in segments:\n",
    "            features = self.feature_extractor(\n",
    "                segment, \n",
    "                sampling_rate=self.sample_rate,\n",
    "                max_length=self.segment_length_samples,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "            segment_features.append(features.input_values[0])\n",
    "        \n",
    "        segment_features = torch.stack(segment_features)  # (num_segments, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'input_values': segment_features, \n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'num_segments': len(segments)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73290837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPoolingLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, seq_len, embed_dim).\n",
    "            mask: The padding mask of shape (batch_size, seq_len).\n",
    "        Returns:\n",
    "            The output tensor of shape (batch_size, embed_dim).\n",
    "        \"\"\"\n",
    "        weights = self.linear(x)  # (bs, seq_len, embed_dim) -> (bs, seq_len, 1)\n",
    "\n",
    "        # Apply the mask before softmax to ignore padding\n",
    "        if mask is not None:\n",
    "            # .unsqueeze(-1): (bs, seq_len) -> (bs, seq_len, 1)\n",
    "            # Assign a very negative value where the mask is True (padding)\n",
    "            weights.masked_fill_(mask.unsqueeze(-1), -1e9)\n",
    "\n",
    "        weights = torch.softmax(weights, dim=1)  # Now masked elements will have ~0 weight\n",
    "\n",
    "        # Weighted sum (bs, seq_len, 1) * (bs, seq_len, embed_dim) -> (bs, embed_dim)\n",
    "        x = torch.sum(weights * x, dim=1) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89dacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressionClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout=0.1, \n",
    "                 seq_model_type='bilstm', seq_hidden_size=256):\n",
    "        super(DepressionClassifier, self).__init__()\n",
    "    \n",
    "        # SSL model loading & config\n",
    "        self.ssl_model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.ssl_hidden_size = self.ssl_model.config.hidden_size # e.g. 768\n",
    "\n",
    "        # Weighted sum of SSL model's hidden layers\n",
    "        num_ssl_layers = self.ssl_model.config.num_hidden_layers\n",
    "        layers_to_aggregate = num_ssl_layers + 1 # +1 for the initial embeddings\n",
    "\n",
    "        self.layer_weights = nn.Parameter(torch.ones(layers_to_aggregate))\n",
    "        self.layer_norms = nn.ModuleList(\n",
    "            [nn.LayerNorm(self.ssl_hidden_size) for _ in range(layers_to_aggregate)]\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Segment-level pooling\n",
    "        self.segment_embeddings_pooling = AttentionPoolingLayer(embed_dim=self.ssl_hidden_size)\n",
    "        self.segment_embedding_dim = self.ssl_hidden_size \n",
    "\n",
    "        self.seq_model_type = seq_model_type\n",
    "\n",
    "        if self.seq_model_type == 'bilstm':\n",
    "            self.sequence_model = nn.LSTM(\n",
    "                input_size=self.ssl_hidden_size,\n",
    "                hidden_size=seq_hidden_size,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            self.seq_output_dim = seq_hidden_size * 2  # bidirectional\n",
    "        elif self.seq_model_type == 'transformer':\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=self.ssl_hidden_size,\n",
    "                nhead=8,  # nhead must be a divisor of ssl_hidden_size (e.g. 768 % 8 == 0)\n",
    "                dim_feedforward=seq_hidden_size * 2, # Common practice\n",
    "                dropout=dropout,\n",
    "                activation='relu',\n",
    "                batch_first=True \n",
    "            )\n",
    "            self.sequence_model = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "            self.seq_output_dim = self.ssl_hidden_size\n",
    "\n",
    "        self.audio_embedding_pooling = AttentionPoolingLayer(embed_dim=self.seq_output_dim)\n",
    "        self.audio_embedding_dim = self.seq_output_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.audio_embedding_dim, self.ssl_hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.ssl_hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # initialize weights of classifier\n",
    "        for name, param in self.classifier.named_parameters():\n",
    "            if 'weight' in name and len(param.shape) > 1:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_values = batch['input_values']\n",
    "        attention_mask = batch.get('attention_mask', None) \n",
    "        batch_size, num_segments, seq_len = input_values.shape\n",
    "        \n",
    "        # Reshape from (bs, num_segments, seq_len) to (bs * num_segments, seq_len)\n",
    "        # This allows processing all segments from all audio files in one go.\n",
    "        input_values_flat = input_values.view(batch_size * num_segments, seq_len)\n",
    "        \n",
    "        ssl_hidden_states = self.ssl_model(\n",
    "            input_values=input_values_flat,\n",
    "            return_dict=True,\n",
    "        ).hidden_states  # (bs * num_segments, num_frames, hidden_size)\n",
    "\n",
    "        # Combine all hidden layers from the SSL model using learned weights.\n",
    "        ssl_hidden_state = torch.zeros_like(ssl_hidden_states[-1])  # (bs * num_segments, num_frames, hidden_size)\n",
    "        weights = self.softmax(self.layer_weights)\n",
    "        for i in range(len(ssl_hidden_states)):\n",
    "            ssl_hidden_state += weights[i] * self.layer_norms[i](ssl_hidden_states[i])\n",
    "\n",
    "        # Pool the sequence of frames into a single representation for the whole segment.\n",
    "        segment_embeddings_flat = self.segment_embeddings_pooling(ssl_hidden_state)  # (bs * num_segments, segment_embedding_dim)\n",
    "\n",
    "        # Un-flatten the batch to restore sequence structure \n",
    "        # Reshape from (bs * num_segments, segment_embedding_dim) back to (bs, num_segments, segment_embedding_dim)\n",
    "        segment_embeddings = segment_embeddings_flat.view(batch_size, num_segments, self.segment_embedding_dim)\n",
    "\n",
    "        # Sequence modeling across segments\n",
    "        # Process the sequence of segment embeddings for each audio file.\n",
    "        if self.sequence_model_type == 'bilstm':\n",
    "            sequence_output, _ = self.sequence_model(segment_embeddings)\n",
    "        elif self.sequence_model_type == 'transformer':\n",
    "            sequence_output = self.sequence_model(segment_embeddings, src_key_padding_mask=attention_mask)\n",
    "        # Result shape: (bs, num_segments, seq_output_dim)\n",
    "        \n",
    "        # Pool the sequence of segments into a single representation for the whole audio file.\n",
    "        audio_embeddings = self.audio_embedding_pooling(sequence_output, mask=attention_mask)  # (bs, audio_embedding_dim)\n",
    "\n",
    "        output = self.classifier(audio_embeddings)  # (bs, num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efceb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Questa funzione serve perché diversi audio possono avere numero diverso di segmenti.\n",
    "    Ad esempio:\n",
    "    - Audio 1: 30 secondi → 3 segmenti da 10s\n",
    "    - Audio 2: 50 secondi → 5 segmenti da 10s\n",
    "    \n",
    "    Per creare un batch uniforme, dobbiamo fare padding al numero massimo di segmenti.\n",
    "    Viene chiamata automaticamente dal DataLoader quando batch_size > 1.\n",
    "    \"\"\"\n",
    "    # Trova il numero massimo di segmenti nel batch\n",
    "    max_segments = max([item['num_segments'] for item in batch])\n",
    "    \n",
    "    batch_input_values = []\n",
    "    batch_labels = []\n",
    "    batch_masks = []\n",
    "    \n",
    "    for item in batch:\n",
    "        input_values = item['input_values']\n",
    "        num_segments = item['num_segments']\n",
    "\n",
    "        mask = torch.zeros(max_segments, dtype=torch.bool)\n",
    "        mask[num_segments:] = True\n",
    "        batch_masks.append(mask)\n",
    "\n",
    "        # Pad se necessario (aggiunge segmenti di zeri)\n",
    "        if num_segments < max_segments:\n",
    "            padding_shape = (max_segments - num_segments, input_values.shape[1])\n",
    "            padding = torch.zeros(padding_shape, dtype=input_values.dtype)\n",
    "            input_values = torch.cat([input_values, padding], dim=0)\n",
    "        \n",
    "        batch_input_values.append(input_values)\n",
    "        batch_labels.append(item['label'])\n",
    "\n",
    "    return {\n",
    "        'input_values': torch.stack(batch_input_values),\n",
    "        'label': torch.stack(batch_labels),\n",
    "        'attention_mask': torch.stack(batch_masks)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/wav2vec2-base\"\n",
    "num_classes = 2 \n",
    "dataset_name = \"datasets/DAIC-WOZ-Cleaned\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join('datasets', dataset_name, 'train_split_Depression_AVEC2017.csv'))\n",
    "dev_df = pd.read_csv(os.path.join('datasets', dataset_name, 'dev_split_Depression_AVEC2017.csv'))\n",
    "test_df = pd.read_csv(os.path.join('datasets', dataset_name, 'full_test_split.csv'))\n",
    "\n",
    "y_train = load_labels_from_dataset(train_df)\n",
    "y_dev = load_labels_from_dataset(dev_df) \n",
    "y_test = load_labels_from_dataset(test_df)\n",
    "\n",
    "train_paths = get_audio_paths(train_df, dataset_name)\n",
    "dev_paths = get_audio_paths(dev_df, dataset_name)\n",
    "test_paths = get_audio_paths(test_df, dataset_name)\n",
    "\n",
    "# Datasets\n",
    "segment_length_seconds = 10  # Segmenti da 10 secondi\n",
    "max_segments = 150  # Massimo 150 segmenti (25 minuti max)\n",
    "\n",
    "train_dataset = AudioDepressionDataset(\n",
    "    audio_paths=train_paths,\n",
    "    labels=y_train,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=segment_length_seconds,\n",
    "    max_segments=max_segments\n",
    ")\n",
    "\n",
    "dev_dataset = AudioDepressionDataset(\n",
    "    audio_paths=dev_paths,\n",
    "    labels=y_dev,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=segment_length_seconds,\n",
    "    max_segments=max_segments   \n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Modello\n",
    "model = DepressionClassifier(\n",
    "    model_name=model_name,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1,\n",
    "    seq_model_type='bilstm',  # Prova anche 'transformer'\n",
    "    seq_hidden_size=64\n",
    ")\n",
    "\n",
    "# Ottimizzatore e loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaf247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "model_save_path = \"depression_classifier_best.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_pbar = tqdm(enumerate(train_dataloader), \n",
    "                      total=len(train_dataloader),\n",
    "                      desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in train_pbar:\n",
    "        batch['input_values'] = batch['input_values'].to(device)\n",
    "        batch['label'] = batch['label'].to(device)\n",
    "        if 'attention_mask' in batch:\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch['label'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            batch['input_values'] = batch['input_values'].to(device)\n",
    "            batch['label'] = batch['label'].to(device)\n",
    "            if 'attention_mask' in batch:\n",
    "                batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            \n",
    "            output = model(batch)\n",
    "            val_loss += criterion(output, batch['label']).item()\n",
    "            \n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            total += batch['label'].size(0)\n",
    "            correct += (predictions == batch['label']).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(dev_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}')\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved to {model_save_path} with accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test_dataset = AudioDepressionDataset(\n",
    "    audio_paths=test_paths,\n",
    "    labels=y_test,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=10,\n",
    "    max_segments=150\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
