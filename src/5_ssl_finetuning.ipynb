{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "from transformers import AutoModel\n",
    "\n",
    "from utils import load_labels_from_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDepressionDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, num_samples=None):\n",
    "        self.audio_paths = audio_paths  # Lista dei percorsi ai file .wav\n",
    "        self.labels = labels            # Lista di etichette (0 o 1)\n",
    "        self.num_samples = num_samples  # Lunghezza desiderata dei segnali (in campioni)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Carica l'audio\n",
    "        waveform, _ = torchaudio.load(audio_path)\n",
    "\n",
    "        # Pad o taglia per avere lunghezza fissa\n",
    "        if self.num_samples:\n",
    "            if waveform.shape[1] > self.num_samples:\n",
    "                waveform = waveform[:, :self.num_samples]\n",
    "            elif waveform.shape[1] < self.num_samples:\n",
    "                padding = self.num_samples - waveform.shape[1]\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "        return {\n",
    "            'input': waveform,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89dacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "class LayerWeightedAttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        # Pesi per somma tra layer\n",
    "        self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)\n",
    "        # Attention pooling: wᵗ·tanh(W·hᵢ)\n",
    "        self.attn_proj = nn.Linear(hidden_size, 128)\n",
    "        self.attn_score = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: list of tensors (num_layers) of shape (B, T, H)\n",
    "        stacked = torch.stack(hidden_states, dim=0)  # (L, B, T, H)\n",
    "        norm_weights = torch.softmax(self.layer_weights, dim=0)\n",
    "        weighted = (norm_weights[:, None, None, None] * stacked).sum(dim=0)  # (B, T, H)\n",
    "\n",
    "        # Attention pooling\n",
    "        attn = torch.tanh(self.attn_proj(weighted))  # (B, T, 128)\n",
    "        attn_weights = torch.softmax(self.attn_score(attn), dim=1)  # (B, T, 1)\n",
    "        pooled = (weighted * attn_weights).sum(dim=1)  # (B, H)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "class DepressionClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout):\n",
    "        super(DepressionClassifier, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        self.ssl_model = AutoModel.from_pretrained(self.model_name, output_hidden_states=True)\n",
    "        \n",
    "        # TODO aggiungere codice per freezare layer se serve\n",
    "        \n",
    "        print(f'Number of trainable parameters: {sum(p.numel() for p in self.ssl_model.parameters() if p.requires_grad) / 1e6:.2f}M')\n",
    "        \n",
    "        # +1 perchè prendiamo anche il layer che fa feature extraction\n",
    "        layers_to_aggregate = self.ssl_model.num_hidden_layers + 1\n",
    "        self.layer_weights = nn.Parameter(torch.ones(layers_to_aggregate))\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(self.ssl_model.hidden_size) for _ in range(layers_to_aggregate)\n",
    "        ])\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
