{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529d4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import load_labels_from_dataset, get_audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2074e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDepressionDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, model_name, sample_rate=16_000, segment_length_seconds=20, max_segments=None):\n",
    "        self.audio_paths = audio_paths  \n",
    "        self.labels = labels            \n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name, do_normalize=False)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length_seconds = segment_length_seconds\n",
    "        self.segment_length_samples = segment_length_seconds * sample_rate\n",
    "        self.max_segments = max_segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def _load_audio(self, audio_path):\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = audio.mean(axis=0)\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        return audio\n",
    "    \n",
    "    def _segment_audio(self, audio):\n",
    "        \"\"\"Segmenta l'audio in chunks di lunghezza fissa\"\"\"\n",
    "        segments = []\n",
    "        \n",
    "        # Se l'audio è più corto del segmento desiderato, pad con zeri\n",
    "        if len(audio) < self.segment_length_samples:\n",
    "            padded_audio = np.zeros(self.segment_length_samples)\n",
    "            padded_audio[:len(audio)] = audio\n",
    "            segments.append(padded_audio)\n",
    "        else:\n",
    "            # Dividi in segmenti\n",
    "            for i in range(0, len(audio), self.segment_length_samples):\n",
    "                segment = audio[i:i + self.segment_length_samples]\n",
    "                \n",
    "                # Se l'ultimo segmento è troppo corto, pad con zeri\n",
    "                if len(segment) < self.segment_length_samples:\n",
    "                    padded_segment = np.zeros(self.segment_length_samples)\n",
    "                    padded_segment[:len(segment)] = segment\n",
    "                    segment = padded_segment\n",
    "                \n",
    "                segments.append(segment)\n",
    "                \n",
    "                # Limita il numero di segmenti se specificato\n",
    "                if self.max_segments and len(segments) >= self.max_segments:\n",
    "                    break\n",
    "        \n",
    "        return np.array(segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        audio = self._load_audio(audio_path)\n",
    "        segments = self._segment_audio(audio)\n",
    "        \n",
    "        segment_features = []\n",
    "        for segment in segments:\n",
    "            features = self.feature_extractor(\n",
    "                segment, \n",
    "                sampling_rate=self.sample_rate,\n",
    "                max_length=self.segment_length_samples,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "                return_attention_mask=False,\n",
    "            )\n",
    "            segment_features.append(features.input_values[0])\n",
    "        \n",
    "        segment_features = torch.stack(segment_features)  # (num_segments, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'input_values': segment_features, \n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'num_segments': len(segments)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73290837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveStatisticsPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Attentive Statistics Pooling based on\n",
    "    \"Attentive Statistics Pooling for Deep Speaker Embedding\" (https://www.isca-archive.org/interspeech_2018/okabe18_interspeech.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, attention_dim=64):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, attention_dim)\n",
    "        # BatchNorm1d is applied on the channel dimension\n",
    "        self.bn = nn.BatchNorm1d(attention_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, seq_len, input_dim).\n",
    "        Returns:\n",
    "            The output tensor of shape (batch_size, input_dim * 2).\n",
    "        \"\"\"\n",
    "        # (batch_size, seq_len, input_dim) -> (batch_size, seq_len, attention_dim)\n",
    "        x_attn = self.linear1(x)\n",
    "\n",
    "        # BatchNorm requires shape (batch_size, channels, seq_len), so we transpose\n",
    "        # (batch_size, seq_len, attention_dim) -> (batch_size, attention_dim, seq_len)\n",
    "        x_attn = x_attn.transpose(1, 2)\n",
    "        x_attn = self.bn(x_attn)\n",
    "        # Transpose back to the original dimension order\n",
    "        # (batch_size, attention_dim, seq_len) -> (batch_size, seq_len, attention_dim)\n",
    "        x_attn = x_attn.transpose(1, 2)\n",
    "\n",
    "        # Apply activation and final linear layer\n",
    "        x_attn = self.relu(x_attn)\n",
    "        # (batch_size, seq_len, attention_dim) -> (batch_size, seq_len, 1)\n",
    "        attention_scores = self.linear2(x_attn)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "\n",
    "        # Equation (5): Weighted mean\n",
    "        # (batch_size, seq_len, 1) * (batch_size, seq_len, input_dim) -> (batch_size, input_dim)\n",
    "        mean = torch.sum(attention_weights * x, dim=1)\n",
    "\n",
    "        # Equation (6): Weighted standard deviation\n",
    "        # E[X^2] - (E[X])^2\n",
    "        # (batch_size, seq_len, 1) * (batch_size, seq_len, input_dim) -> (batch_size, input_dim)\n",
    "        variance = torch.sum(attention_weights * x.pow(2), dim=1) - mean.pow(2)\n",
    "        std_dev = torch.sqrt(variance.clamp(min=1e-6))\n",
    "\n",
    "        # (batch_size, input_dim), (batch_size, input_dim) -> (batch_size, input_dim * 2)\n",
    "        pooled_output = torch.cat((mean, std_dev), dim=1)\n",
    "\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89dacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressionClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout=0.1, \n",
    "                 sequence_model_type='bilstm', sequence_hidden_size=256):\n",
    "        super(DepressionClassifier, self).__init__()\n",
    "    \n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        self.sequence_model_type = sequence_model_type\n",
    "\n",
    "        self.ssl_model = AutoModel.from_pretrained(self.model_name, output_hidden_states=True)\n",
    "        self.ssl_hidden_size = self.ssl_model.config.hidden_size\n",
    "        self.head_hidden_size = self.ssl_hidden_size\n",
    "        \n",
    "        # +1 perchè prendiamo anche il layer che fa feature extraction\n",
    "        layers_to_aggregate = self.ssl_model.config.num_hidden_layers + 1\n",
    "        self.layer_weights = nn.Parameter(torch.ones(layers_to_aggregate))\n",
    "        self.layer_norms = nn.ModuleList([\n",
    "            nn.LayerNorm(self.ssl_hidden_size) for _ in range(layers_to_aggregate)\n",
    "        ])\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Segment-level pooling\n",
    "        self.segment_pooling = AttentiveStatisticsPooling(input_dim=self.ssl_hidden_size)\n",
    "        segment_embedding_dim = self.ssl_hidden_size * 2\n",
    "\n",
    "        # Sequence model per aggregare i segmenti\n",
    "        if sequence_model_type == 'bilstm':\n",
    "            self.sequence_model = nn.LSTM(\n",
    "                input_size=segment_embedding_dim,\n",
    "                hidden_size=sequence_hidden_size,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            sequence_output_dim = sequence_hidden_size * 2  # bidirectional\n",
    "        elif sequence_model_type == 'transformer':\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=segment_embedding_dim,\n",
    "                nhead=8,\n",
    "                dim_feedforward=sequence_hidden_size,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.sequence_model = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "            sequence_output_dim = segment_embedding_dim\n",
    "\n",
    "        # Global pooling\n",
    "        self.global_pooling = AttentiveStatisticsPooling(input_dim=sequence_output_dim)\n",
    "        global_embedding_dim = sequence_output_dim * 2\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(global_embedding_dim, self.head_hidden_size),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.head_hidden_size, self.num_classes),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # initialize weights of classifier\n",
    "        for name, param in self.classifier.named_parameters():\n",
    "            if 'weight' in name and len(param.shape) > 1:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # --- Step 1: Unpack batch and get dimensions ---\n",
    "        input_values = batch['input_values']      # (batch_size, num_segments, seq_len)\n",
    "        attention_mask = batch.get('attention_mask', None) # (batch_size, num_segments), True for padding\n",
    "        \n",
    "        batch_size, num_segments, seq_len = input_values.shape\n",
    "        \n",
    "        # --- Step 2: Flatten the batch for efficient SSL processing ---\n",
    "        # Reshape from (batch, segs, len) to (batch * segs, len)\n",
    "        # This allows processing all segments from all audio files in one go.\n",
    "        input_values_flat = input_values.view(batch_size * num_segments, seq_len)\n",
    "        \n",
    "        # --- Step 3: Process through the Self-Supervised Learning (SSL) model ---\n",
    "        ssl_outputs = self.ssl_model(\n",
    "            input_values=input_values_flat,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        ssl_hidden_states = ssl_outputs.hidden_states # Tuple of (batch * segs, ssl_seq_len, hidden_size)\n",
    "\n",
    "        # --- Step 4: Weighted layer aggregation ---\n",
    "        # Combine all hidden layers from the SSL model using learned weights.\n",
    "        ssl_hidden_state = torch.zeros_like(ssl_hidden_states[-1])\n",
    "        weights = self.softmax(self.layer_weights)\n",
    "        for i in range(len(ssl_hidden_states)):\n",
    "            ssl_hidden_state += weights[i] * self.layer_norms[i](ssl_hidden_states[i])\n",
    "        # Result shape: (batch * segs, ssl_seq_len, hidden_size)\n",
    "\n",
    "        # --- Step 5: Segment-level pooling ---\n",
    "        # Pool the time dimension of each segment into a single vector.\n",
    "        segment_embeddings_flat = self.segment_pooling(ssl_hidden_state)\n",
    "        # Result shape: (batch * segs, hidden_size * 2)\n",
    "        \n",
    "        # --- Step 6: Un-flatten the batch to restore sequence structure ---\n",
    "        # Reshape from (batch * segs, embed_dim) back to (batch, segs, embed_dim)\n",
    "        segment_embedding_dim = segment_embeddings_flat.shape[-1]\n",
    "        all_segment_embeddings = segment_embeddings_flat.view(batch_size, num_segments, segment_embedding_dim)\n",
    "\n",
    "        # --- Step 7: Sequence modeling across segments ---\n",
    "        # Process the sequence of segment embeddings for each audio file.\n",
    "        if self.sequence_model_type == 'bilstm':\n",
    "            # Note: BiLSTM does not natively use the mask, so it will process padded segments.\n",
    "            # This is a simplification; for optimal performance, pack_padded_sequence could be used.\n",
    "            sequence_output, _ = self.sequence_model(all_segment_embeddings)\n",
    "        elif self.sequence_model_type == 'transformer':\n",
    "            # The Transformer Encoder correctly uses the mask to ignore padded segments.\n",
    "            sequence_output = self.sequence_model(all_segment_embeddings, src_key_padding_mask=attention_mask)\n",
    "        # Result shape: (batch, segs, sequence_output_dim)\n",
    "        \n",
    "        # --- Step 8: Global pooling across all segments ---\n",
    "        # Pool the sequence of segments into a single representation for the whole audio file.\n",
    "        # We pass the attention_mask to ignore padded segments during pooling.\n",
    "        global_embeddings = self.global_pooling(sequence_output)\n",
    "        # Result shape: (batch, sequence_output_dim * 2)\n",
    "        \n",
    "        # --- Step 9: Final classification ---\n",
    "        output = self.classifier(global_embeddings)\n",
    "        # Result shape: (batch, num_classes)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efceb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione di collate per gestire batch con numero variabile di segmenti\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Questa funzione serve perché diversi audio possono avere numero diverso di segmenti.\n",
    "    Ad esempio:\n",
    "    - Audio 1: 30 secondi → 3 segmenti da 10s\n",
    "    - Audio 2: 50 secondi → 5 segmenti da 10s\n",
    "    \n",
    "    Per creare un batch uniforme, dobbiamo fare padding al numero massimo di segmenti.\n",
    "    Viene chiamata automaticamente dal DataLoader quando batch_size > 1.\n",
    "    \"\"\"\n",
    "    # Trova il numero massimo di segmenti nel batch\n",
    "    max_segments = max([item['num_segments'] for item in batch])\n",
    "    \n",
    "    batch_input_values = []\n",
    "    batch_labels = []\n",
    "    batch_attention_mask = []\n",
    "    \n",
    "    for item in batch:\n",
    "        input_values = item['input_values']\n",
    "        num_segments = item['num_segments']\n",
    "\n",
    "        mask = torch.zeros(max_segments, dtype=torch.bool)\n",
    "        mask[:num_segments] = False # Segmenti reali\n",
    "        mask[num_segments:] = True # Segmenti di padding\n",
    "        \n",
    "        # Pad se necessario (aggiunge segmenti di zeri)\n",
    "        if num_segments < max_segments:\n",
    "            padding_shape = (max_segments - num_segments, input_values.shape[1])\n",
    "            padding = torch.zeros(padding_shape, dtype=input_values.dtype)\n",
    "            input_values = torch.cat([input_values, padding], dim=0)\n",
    "        \n",
    "        batch_input_values.append(input_values)\n",
    "        batch_labels.append(item['label'])\n",
    "        batch_attention_mask.append(mask)\n",
    "\n",
    "    return {\n",
    "        'input_values': torch.stack(batch_input_values),\n",
    "        'label': torch.stack(batch_labels),\n",
    "        'attention_mask': torch.stack(batch_attention_mask) # (batch_size, max_segments)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c04f601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidebonura/miniconda3/lib/python3.12/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Summary ===\n",
      "Total parameters: 95,568,017\n",
      "Trainable parameters: 95,568,017\n"
     ]
    }
   ],
   "source": [
    "# Parametri\n",
    "model_name = \"facebook/wav2vec2-base\"\n",
    "num_classes = 2  # Binary classification (depressed vs non-depressed)\n",
    "dataset_name = \"datasets/DAIC-WOZ-Cleaned\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'train_split_Depression_AVEC2017.csv'))\n",
    "dev_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'dev_split_Depression_AVEC2017.csv'))\n",
    "test_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'full_test_split.csv'))\n",
    "\n",
    "y_train = load_labels_from_dataset(train_df)\n",
    "y_dev = load_labels_from_dataset(dev_df) \n",
    "y_test = load_labels_from_dataset(test_df)\n",
    "\n",
    "# Carica i path audio\n",
    "train_paths = get_audio_paths(train_df, dataset_name)\n",
    "dev_paths = get_audio_paths(dev_df, dataset_name)\n",
    "test_paths = get_audio_paths(test_df, dataset_name)\n",
    "\n",
    "# Crea i dataset\n",
    "train_dataset = AudioDepressionDataset(\n",
    "    audio_paths=train_paths,\n",
    "    labels=y_train,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=4,  # Segmenti da 10 secondi\n",
    "    max_segments=20  # Massimo 140 segmenti (= 23.3 minuti max)\n",
    ")\n",
    "\n",
    "dev_dataset = AudioDepressionDataset(\n",
    "    audio_paths=dev_paths,\n",
    "    labels=y_dev,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=4,\n",
    "    max_segments=20\n",
    ")\n",
    "\n",
    "test_dataset = AudioDepressionDataset(\n",
    "    audio_paths=test_paths,\n",
    "    labels=y_test,\n",
    "    model_name=model_name,\n",
    "    segment_length_seconds=4,\n",
    "    max_segments=20 \n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=2,  # Riduci se hai problemi di memoria\n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0   \n",
    ")\n",
    "\n",
    "'''\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "'''\n",
    "\n",
    "# Modello\n",
    "model = DepressionClassifier(\n",
    "    model_name=model_name,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1,\n",
    "    sequence_model_type='bilstm',  # Prova anche 'transformer'\n",
    "    sequence_hidden_size=64\n",
    ")\n",
    "\n",
    "# Ottimizzatore e loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaf247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training:   0%|          | 0/54 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_pbar = tqdm(enumerate(train_dataloader), \n",
    "                      total=len(train_dataloader),\n",
    "                      desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    for batch_idx, batch in train_pbar:\n",
    "        batch['input_values'] = batch['input_values'].to(device)\n",
    "        batch['label'] = batch['label'].to(device)\n",
    "        batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch['label'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            batch['input_values'] = batch['input_values'].to(device)\n",
    "            batch['label'] = batch['label'].to(device)\n",
    "            \n",
    "            output = model(batch)\n",
    "            val_loss += criterion(output, batch['label']).item()\n",
    "            \n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            total += batch['label'].size(0)\n",
    "            correct += (predictions == batch['label']).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(dev_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
