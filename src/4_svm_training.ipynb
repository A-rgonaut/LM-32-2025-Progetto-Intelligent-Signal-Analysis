{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ea7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "from utils import load_labels_from_dataset, load_features_from_dataset\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13faa5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label distribution: [77 30]\n",
      "Dev label distribution: [23 12]\n",
      "Test label distribution: [33 14]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'train_split_Depression_AVEC2017.csv'))\n",
    "dev_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'dev_split_Depression_AVEC2017.csv'))\n",
    "test_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'full_test_split.csv'))\n",
    "\n",
    "dataset_name = \"DAIC-WOZ-Cleaned\"\n",
    "\n",
    "y_train = load_labels_from_dataset(train_df)\n",
    "y_dev = load_labels_from_dataset(dev_df) \n",
    "y_test = load_labels_from_dataset(test_df)\n",
    "\n",
    "X_train_artic = load_features_from_dataset(train_df, dataset_name, 'articulation')\n",
    "X_dev_artic = load_features_from_dataset(dev_df, dataset_name, 'articulation') \n",
    "X_test_artic = load_features_from_dataset(test_df, dataset_name, 'articulation')\n",
    "\n",
    "X_train_phona = load_features_from_dataset(train_df, dataset_name, 'phonation')\n",
    "X_dev_phona = load_features_from_dataset(dev_df, dataset_name, 'phonation') \n",
    "X_test_phona = load_features_from_dataset(test_df, dataset_name, 'phonation')\n",
    "\n",
    "X_train_proso = load_features_from_dataset(train_df, dataset_name, 'prosody')\n",
    "X_dev_proso = load_features_from_dataset(dev_df, dataset_name, 'prosody')\n",
    "X_test_proso = load_features_from_dataset(test_df, dataset_name, 'prosody')\n",
    "\n",
    "print(f\"Training label distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Dev label distribution: {np.bincount(y_dev)}\")\n",
    "print(f\"Test label distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "imputer_proso = SimpleImputer(strategy='mean')\n",
    "X_train_proso = imputer_proso.fit_transform(X_train_proso)\n",
    "X_dev_proso = imputer_proso.transform(X_dev_proso) \n",
    "X_test_proso = imputer_proso.transform(X_test_proso)\n",
    "\n",
    "# Scale features\n",
    "scaler_artic = StandardScaler()\n",
    "X_train_artic_scaled = scaler_artic.fit_transform(X_train_artic)\n",
    "X_dev_artic_scaled = scaler_artic.transform(X_dev_artic) \n",
    "X_test_artic_scaled = scaler_artic.transform(X_test_artic)\n",
    "\n",
    "scaler_phona = StandardScaler()\n",
    "X_train_phona_scaled = scaler_phona.fit_transform(X_train_phona)\n",
    "X_dev_phona_scaled = scaler_phona.transform(X_dev_phona) \n",
    "X_test_phona_scaled = scaler_phona.transform(X_test_phona)\n",
    "\n",
    "scaler_proso = StandardScaler()\n",
    "X_train_proso_scaled = scaler_proso.fit_transform(X_train_proso)\n",
    "X_dev_proso_scaled = scaler_proso.transform(X_dev_proso)\n",
    "X_test_proso_scaled = scaler_proso.transform(X_test_proso)\n",
    "\n",
    "# Create baseline by fusing all three feature types\n",
    "X_train_baseline = np.concatenate([X_train_artic_scaled, X_train_phona_scaled, X_train_proso_scaled], axis=1)\n",
    "X_dev_baseline = np.concatenate([X_dev_artic_scaled, X_dev_phona_scaled, X_dev_proso_scaled], axis=1)\n",
    "X_test_baseline = np.concatenate([X_test_artic_scaled, X_test_phona_scaled, X_test_proso_scaled], axis=1)\n",
    "\n",
    "scaler_baseline = StandardScaler()\n",
    "X_train_baseline_scaled = scaler_baseline.fit_transform(X_train_baseline)\n",
    "X_dev_baseline_scaled = scaler_baseline.fit_transform(X_dev_baseline) \n",
    "X_test_baseline_scaled = scaler_baseline.fit_transform(X_test_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd97d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train, X_dev, y_dev):\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "        'gamma': [1e-3, 1e-2, 1e-1, 1, 'scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'class_weight': ['balanced']\n",
    "    }\n",
    "\n",
    "    for C in param_grid['C']:\n",
    "        for gamma in param_grid['gamma']:\n",
    "            for kernel in param_grid['kernel']:\n",
    "                for class_weight in param_grid['class_weight']:\n",
    "                    if kernel == 'linear' and gamma not in ['scale', 'auto']:\n",
    "                        continue  # Linear non usa gamma\n",
    "\n",
    "                    # Addestra sul training set\n",
    "                    svm = SVC(C=C, gamma=gamma, kernel=kernel, random_state=42, class_weight=class_weight)\n",
    "                    svm.fit(X_train, y_train)\n",
    "\n",
    "                    # Valuta sul dev set\n",
    "                    y_dev_pred = svm.predict(X_dev)\n",
    "                    dev_accuracy = accuracy_score(y_dev, y_dev_pred)\n",
    "\n",
    "                    if dev_accuracy > best_score:\n",
    "                        best_score = dev_accuracy\n",
    "                        best_params = {'C': C, 'gamma': gamma, 'kernel': kernel, 'class_weight': class_weight}\n",
    "                        best_model = svm\n",
    "                        \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best dev accuracy: {best_score:.3f}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b862ccc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_train = np.concatenate([y_train, y_dev], axis=0)\\nfull_train_df = pd.concat([train_df, dev_df], ignore_index=True)\\nX_train_artic = load_features_from_dataset(full_train_df, dataset_name, \\'articulation\\')\\nX_test_artic = load_features_from_dataset(test_df, dataset_name, \\'articulation\\')\\n\\nX_train_phona = load_features_from_dataset(full_train_df, dataset_name, \\'phonation\\')\\nX_test_phona = load_features_from_dataset(test_df, dataset_name, \\'phonation\\')\\n\\nX_train_proso = load_features_from_dataset(full_train_df, dataset_name, \\'prosody\\')\\nX_test_proso = load_features_from_dataset(test_df, dataset_name, \\'prosody\\')\\n\\nimputer_proso = SimpleImputer(strategy=\\'mean\\')\\nX_train_proso = imputer_proso.fit_transform(X_train_proso)\\nX_test_proso = imputer_proso.transform(X_test_proso)\\nscaler_artic = StandardScaler()\\nX_train_artic_scaled = scaler_artic.fit_transform(X_train_artic)\\nX_test_artic_scaled = scaler_artic.transform(X_test_artic)\\nscaler_phona = StandardScaler()\\nX_train_phona_scaled = scaler_phona.fit_transform(X_train_phona)\\nX_test_phona_scaled = scaler_phona.transform(X_test_phona)\\nscaler_proso = StandardScaler()\\nX_train_proso_scaled = scaler_proso.fit_transform(X_train_proso)\\nX_test_proso_scaled = scaler_proso.transform(X_test_proso)\\nX_train_baseline = np.concatenate([X_train_artic_scaled, X_train_phona_scaled, X_train_proso_scaled], axis=1)\\nX_test_baseline = np.concatenate([X_test_artic_scaled, X_test_phona_scaled, X_test_proso_scaled], axis=1)\\nscaler_baseline = StandardScaler()\\nX_train_baseline_scaled = scaler_baseline.fit_transform(X_train_baseline)\\nX_test_baseline_scaled = scaler_baseline.fit_transform(X_test_baseline)\\n\\ndef train_svm(X, y):\\n    param_grid = {\\n        \\'C\\': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\\n        \\'gamma\\': [1e-3, 1e-2, 1e-1, 1, \\'scale\\', \\'auto\\'],\\n        \\'kernel\\': [\\'rbf\\', \\'linear\\'],\\n        \\'class_weight\\': [\\'balanced\\', None]\\n    }\\n\\n    groups = full_train_df[\\'Participant_ID\\']\\n\\n    group_kfold = GroupKFold(n_splits=10)\\n\\n    best_score = 0\\n    best_params = None\\n    best_model = None\\n\\n    for C in param_grid[\\'C\\']:\\n        for gamma in param_grid[\\'gamma\\']:\\n            for kernel in param_grid[\\'kernel\\']:\\n                for class_weight in param_grid[\\'class_weight\\']:\\n                    if kernel == \\'linear\\' and gamma not in [\\'scale\\', \\'auto\\']:\\n                        continue  # \\'gamma\\' non si applica a kernel lineare\\n\\n                    accuracies = []\\n                    \\n                    for train_idx, val_idx in group_kfold.split(X, y, groups):\\n                        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\\n                        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\\n\\n                        model = SVC(C=C, gamma=gamma, kernel=kernel,\\n                                    class_weight=class_weight, random_state=42)\\n                        model.fit(X_train_fold, y_train_fold)\\n                        y_val_pred = model.predict(X_val_fold)\\n                        acc = accuracy_score(y_val_fold, y_val_pred)\\n                        accuracies.append(acc)\\n\\n                    mean_accuracy = np.mean(accuracies)\\n\\n                    if mean_accuracy > best_score:\\n                        best_score = mean_accuracy\\n                        best_params = {\\'C\\': C, \\'gamma\\': gamma, \\'kernel\\': kernel, \\'class_weight\\': class_weight}\\n                        best_model = SVC(C=C, gamma=gamma, kernel=kernel,\\n                                         class_weight=class_weight, random_state=42)\\n                        best_model.fit(X, y)  # retrain su tutto il training set\\n\\n    best_model = SVC(C=C, gamma=gamma, kernel=kernel,\\n                     class_weight=class_weight, random_state=42)\\n    best_model.fit(X, y)  # retrain su tutto il training set\\n    print(f\"\\nBest parameters: {best_params}\")\\n    print(f\"Best mean CV accuracy: {best_score:.3f}\")\\n    return best_model\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "y_train = np.concatenate([y_train, y_dev], axis=0)\n",
    "full_train_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "X_train_artic = load_features_from_dataset(full_train_df, dataset_name, 'articulation')\n",
    "X_test_artic = load_features_from_dataset(test_df, dataset_name, 'articulation')\n",
    "\n",
    "X_train_phona = load_features_from_dataset(full_train_df, dataset_name, 'phonation')\n",
    "X_test_phona = load_features_from_dataset(test_df, dataset_name, 'phonation')\n",
    "\n",
    "X_train_proso = load_features_from_dataset(full_train_df, dataset_name, 'prosody')\n",
    "X_test_proso = load_features_from_dataset(test_df, dataset_name, 'prosody')\n",
    "\n",
    "imputer_proso = SimpleImputer(strategy='mean')\n",
    "X_train_proso = imputer_proso.fit_transform(X_train_proso)\n",
    "X_test_proso = imputer_proso.transform(X_test_proso)\n",
    "scaler_artic = StandardScaler()\n",
    "X_train_artic_scaled = scaler_artic.fit_transform(X_train_artic)\n",
    "X_test_artic_scaled = scaler_artic.transform(X_test_artic)\n",
    "scaler_phona = StandardScaler()\n",
    "X_train_phona_scaled = scaler_phona.fit_transform(X_train_phona)\n",
    "X_test_phona_scaled = scaler_phona.transform(X_test_phona)\n",
    "scaler_proso = StandardScaler()\n",
    "X_train_proso_scaled = scaler_proso.fit_transform(X_train_proso)\n",
    "X_test_proso_scaled = scaler_proso.transform(X_test_proso)\n",
    "X_train_baseline = np.concatenate([X_train_artic_scaled, X_train_phona_scaled, X_train_proso_scaled], axis=1)\n",
    "X_test_baseline = np.concatenate([X_test_artic_scaled, X_test_phona_scaled, X_test_proso_scaled], axis=1)\n",
    "scaler_baseline = StandardScaler()\n",
    "X_train_baseline_scaled = scaler_baseline.fit_transform(X_train_baseline)\n",
    "X_test_baseline_scaled = scaler_baseline.fit_transform(X_test_baseline)\n",
    "\n",
    "def train_svm(X, y):\n",
    "    param_grid = {\n",
    "        'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "        'gamma': [1e-3, 1e-2, 1e-1, 1, 'scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'class_weight': ['balanced', None]\n",
    "    }\n",
    "\n",
    "    groups = full_train_df['Participant_ID']\n",
    "\n",
    "    group_kfold = GroupKFold(n_splits=10)\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for C in param_grid['C']:\n",
    "        for gamma in param_grid['gamma']:\n",
    "            for kernel in param_grid['kernel']:\n",
    "                for class_weight in param_grid['class_weight']:\n",
    "                    if kernel == 'linear' and gamma not in ['scale', 'auto']:\n",
    "                        continue  # 'gamma' non si applica a kernel lineare\n",
    "\n",
    "                    accuracies = []\n",
    "                    \n",
    "                    for train_idx, val_idx in group_kfold.split(X, y, groups):\n",
    "                        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "                        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "                        model = SVC(C=C, gamma=gamma, kernel=kernel,\n",
    "                                    class_weight=class_weight, random_state=42)\n",
    "                        model.fit(X_train_fold, y_train_fold)\n",
    "                        y_val_pred = model.predict(X_val_fold)\n",
    "                        acc = accuracy_score(y_val_fold, y_val_pred)\n",
    "                        accuracies.append(acc)\n",
    "\n",
    "                    mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "                    if mean_accuracy > best_score:\n",
    "                        best_score = mean_accuracy\n",
    "                        best_params = {'C': C, 'gamma': gamma, 'kernel': kernel, 'class_weight': class_weight}\n",
    "                        best_model = SVC(C=C, gamma=gamma, kernel=kernel,\n",
    "                                         class_weight=class_weight, random_state=42)\n",
    "                        best_model.fit(X, y)  # retrain su tutto il training set\n",
    "\n",
    "    best_model = SVC(C=C, gamma=gamma, kernel=kernel,\n",
    "                     class_weight=class_weight, random_state=42)\n",
    "    best_model.fit(X, y)  # retrain su tutto il training set\n",
    "    print(f\"\\nBest parameters: {best_params}\")\n",
    "    print(f\"Best mean CV accuracy: {best_score:.3f}\")\n",
    "    return best_model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dad3479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear', 'class_weight': 'balanced'}\n",
      "Best dev accuracy: 0.657\n",
      "Best parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf', 'class_weight': 'balanced'}\n",
      "Best dev accuracy: 0.714\n",
      "Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear', 'class_weight': 'balanced'}\n",
      "Best dev accuracy: 0.686\n",
      "Best parameters: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf', 'class_weight': 'balanced'}\n",
      "Best dev accuracy: 0.714\n",
      "\n",
      "Classification Report (Art):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.61      0.65        33\n",
      "           1       0.28      0.36      0.31        14\n",
      "\n",
      "    accuracy                           0.53        47\n",
      "   macro avg       0.48      0.48      0.48        47\n",
      "weighted avg       0.57      0.53      0.55        47\n",
      "\n",
      "Sensitivity: 0.357\n",
      "Specificity: 0.606\n",
      "\n",
      "Classification Report (Phon):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.85      0.76        33\n",
      "           1       0.17      0.07      0.10        14\n",
      "\n",
      "    accuracy                           0.62        47\n",
      "   macro avg       0.42      0.46      0.43        47\n",
      "weighted avg       0.53      0.62      0.56        47\n",
      "\n",
      "Sensitivity: 0.071\n",
      "Specificity: 0.071\n",
      "\n",
      "Classification Report (Proso):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.70      0.73        33\n",
      "           1       0.41      0.50      0.45        14\n",
      "\n",
      "    accuracy                           0.64        47\n",
      "   macro avg       0.59      0.60      0.59        47\n",
      "weighted avg       0.66      0.64      0.65        47\n",
      "\n",
      "Sensitivity: 0.500\n",
      "Specificity: 0.697\n",
      "\n",
      "Classification Report (Baseline - Fused Features):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.61      0.65        33\n",
      "           1       0.28      0.36      0.31        14\n",
      "\n",
      "    accuracy                           0.53        47\n",
      "   macro avg       0.48      0.48      0.48        47\n",
      "weighted avg       0.57      0.53      0.55        47\n",
      "\n",
      "Sensitivity: 0.357\n",
      "Specificity: 0.606\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "svm_arti = train_svm(X_train_artic_scaled, y_train, X_dev_artic_scaled, y_dev)\n",
    "svm_phon = train_svm(X_train_phona_scaled, y_train, X_dev_phona_scaled, y_dev)\n",
    "svm_proso = train_svm(X_train_proso_scaled, y_train, X_dev_proso_scaled, y_dev)\n",
    "svm_baseline = train_svm(X_train_baseline_scaled, y_train, X_dev_baseline_scaled, y_dev)\n",
    "\n",
    "'''\n",
    "svm_arti = train_svm(X_train_artic_scaled, y_train)\n",
    "svm_phon = train_svm(X_train_phona_scaled, y_train)\n",
    "svm_proso = train_svm(X_train_proso_scaled, y_train)\n",
    "svm_baseline = train_svm(X_train_baseline_scaled, y_train)\n",
    "'''\n",
    "\n",
    "y_test_pred_artic = svm_arti.predict(X_test_artic_scaled)\n",
    "y_test_pred_phona = svm_phon.predict(X_test_phona_scaled)\n",
    "y_test_pred_proso = svm_proso.predict(X_test_proso_scaled)\n",
    "y_test_pred_baseline = svm_baseline.predict(X_test_baseline_scaled)\n",
    "\n",
    "print(\"\\nClassification Report (Art):\")\n",
    "print(classification_report(y_test, y_test_pred_artic))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred_artic).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Phon):\")\n",
    "print(classification_report(y_test, y_test_pred_phona))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred_phona).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {sensitivity:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Proso):\")\n",
    "print(classification_report(y_test, y_test_pred_proso))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred_proso).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Baseline - Fused Features):\")\n",
    "print(classification_report(y_test, y_test_pred_baseline))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred_baseline).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
