{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1ea7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5f4be90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Percorso corretto per il file\\nfile_path = os.path.join('datasets', 'DAIC-WOZ', 'full_test_split.csv')\\n\\n# Carica il file con le righe vuote\\ndf = pd.read_csv(file_path)\\n\\n# Rimuovi le righe vuote/NaN\\ndf = df.dropna(how='all')\\n\\n# Sovrascrive il file originale pulito\\ndf.to_csv(file_path, index=False)\\n\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Percorso corretto per il file\n",
    "file_path = os.path.join('datasets', 'DAIC-WOZ', 'full_test_split.csv')\n",
    "\n",
    "# Carica il file con le righe vuote\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Rimuovi le righe vuote/NaN\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Sovrascrive il file originale pulito\n",
    "df.to_csv(file_path, index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "200ce01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_for_dataset(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Carica le features di articolazione per un dataset specifico.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame contenente Participant_ID e PHQ8_Binary\n",
    "        dataset_name: Nome della cartella del dataset\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_features, y_labels) come array numpy\n",
    "    \"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    label_col = 'PHQ8_Binary' if 'PHQ8_Binary' in df.columns else 'PHQ_Binary'\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        participant_id = int(row['Participant_ID'])\n",
    "        label = int(row[label_col])  # 0 = non-depressed, 1 = depressed\n",
    "\n",
    "        # Costruisce il percorso delle features\n",
    "        feature_path = os.path.join(\"features\", dataset_name, f\"{participant_id}_P\", \"articulation_features.npy\")\n",
    "        \n",
    "        features = np.load(feature_path)\n",
    "        X_list.append(features.flatten())  \n",
    "        y_list.append(label)\n",
    "    \n",
    "    return np.array(X_list), np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13faa5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 107 samples\n",
      "Dev set: 35 samples\n",
      "Test set: 47 samples\n",
      "Training label distribution: [77 30]\n",
      "Dev label distribution: [23 12]\n",
      "Test label distribution: [33 14]\n"
     ]
    }
   ],
   "source": [
    "# Carica solo i dati di training\n",
    "train_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'train_split_Depression_AVEC2017.csv'))\n",
    "dev_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'dev_split_Depression_AVEC2017.csv'))\n",
    "test_df = pd.read_csv(os.path.join('datasets', 'DAIC-WOZ', 'full_test_split.csv'))\n",
    "\n",
    "dataset_name = \"DAIC-WOZ-Cleaned\"\n",
    "\n",
    "# Carica features per train dev e test separatamente\n",
    "X_train, y_train = load_features_for_dataset(train_df, dataset_name)\n",
    "X_dev, y_dev = load_features_for_dataset(dev_df, dataset_name)\n",
    "X_test, y_test = load_features_for_dataset(test_df, dataset_name)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Dev set: {len(X_dev)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Training label distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Dev label distribution: {np.bincount(y_dev)}\")\n",
    "print(f\"Test label distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_dev_scaled = scaler.transform(X_dev)\n",
    "X_test_scaled = scaler.transform(X_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4dad3479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best dev accuracy: 0.686\n",
      "Final Test Accuracy: 0.660\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.85      0.78        33\n",
      "           1       0.38      0.21      0.27        14\n",
      "\n",
      "    accuracy                           0.66        47\n",
      "   macro avg       0.55      0.53      0.53        47\n",
      "weighted avg       0.62      0.66      0.63        47\n",
      "\n",
      "Sensitivity: 0.214\n",
      "Specificity: 0.848\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - approccio corretto\n",
    "\n",
    "# Parametri da testare\n",
    "param_grid = {\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000],\n",
    "    'gamma': [1e-3, 1e-2, 1e-1, 1, 'scale', 'auto'],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "\n",
    "# Metodo 1: GridSearch con CV sul training, poi valida sul dev\n",
    "best_score = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for C in param_grid['C']:\n",
    "    for gamma in param_grid['gamma']:\n",
    "        for kernel in param_grid['kernel']:\n",
    "            if kernel == 'linear' and gamma not in ['scale', 'auto']:\n",
    "                continue  # Linear non usa gamma\n",
    "            \n",
    "            # Addestra sul training set\n",
    "            svm = SVC(C=C, gamma=gamma, kernel=kernel, random_state=42, class_weight='balanced')\n",
    "            svm.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Valuta sul dev set\n",
    "            y_dev_pred = svm.predict(X_dev_scaled)\n",
    "            dev_accuracy = accuracy_score(y_dev, y_dev_pred)\n",
    "            \n",
    "            if dev_accuracy > best_score:\n",
    "                best_score = dev_accuracy\n",
    "                best_params = {'C': C, 'gamma': gamma, 'kernel': kernel}\n",
    "                best_model = svm\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best dev accuracy: {best_score:.3f}\")\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
