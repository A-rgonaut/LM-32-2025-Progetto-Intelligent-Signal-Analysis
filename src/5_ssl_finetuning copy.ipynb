{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "import numpy as np\n",
    "import librosa\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from utils import load_labels_from_dataset, get_audio_paths, get_split_audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2074e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDepressionDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels, model_name, sample_rate=16_000):\n",
    "        self.audio_paths = audio_paths  \n",
    "        self.labels = labels            \n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name, do_normalize=False)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length_seconds = 10\n",
    "        self.segment_length_samples = self.segment_length_seconds * sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def _load_audio(self, audio_path):\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        if len(audio.shape) > 1:\n",
    "            audio = audio.mean(axis=0)\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        # Pad or truncate to exactly 10s\n",
    "        if len(audio) < self.segment_length_samples:\n",
    "            padded_audio = np.zeros(self.segment_length_samples)\n",
    "            padded_audio[:len(audio)] = audio\n",
    "            audio = padded_audio\n",
    "        else:\n",
    "            audio = audio[:self.segment_length_samples]\n",
    "        return audio\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        audio = self._load_audio(audio_path)\n",
    "        features = self.feature_extractor(\n",
    "            audio, \n",
    "            sampling_rate=self.sample_rate,\n",
    "            max_length=self.segment_length_samples,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_values': features.input_values[0], \n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73290837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPoolingLayer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Args:\n",
    "            x: The input tensor of shape (batch_size, seq_len, embed_dim).\n",
    "            mask: The padding mask of shape (batch_size, seq_len).\n",
    "        Returns:\n",
    "            The output tensor of shape (batch_size, embed_dim).\n",
    "        \"\"\"\n",
    "        weights = self.linear(x)  # (bs, seq_len, embed_dim) -> (bs, seq_len, 1)\n",
    "\n",
    "        # Apply the mask before softmax to ignore padding\n",
    "        if mask is not None:\n",
    "            # .unsqueeze(-1): (bs, seq_len) -> (bs, seq_len, 1)\n",
    "            # Assign a very negative value where the mask is True (padding)\n",
    "            weights.masked_fill_(mask.unsqueeze(-1), -1e9)\n",
    "\n",
    "        weights = torch.softmax(weights, dim=1)  # Now masked elements will have ~0 weight\n",
    "\n",
    "        # Weighted sum (bs, seq_len, 1) * (bs, seq_len, embed_dim) -> (bs, embed_dim)\n",
    "        x = torch.sum(weights * x, dim=1) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89dacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressionClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, dropout=0.1):\n",
    "        super(DepressionClassifier, self).__init__()\n",
    "    \n",
    "        # SSL model loading & config\n",
    "        self.ssl_model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "        self.ssl_hidden_size = self.ssl_model.config.hidden_size # e.g. 768\n",
    "\n",
    "        # Weighted sum of SSL model's hidden layers\n",
    "        num_ssl_layers = self.ssl_model.config.num_hidden_layers\n",
    "        layers_to_aggregate = num_ssl_layers + 1 # +1 for the initial embeddings\n",
    "\n",
    "        self.layer_weights = nn.Parameter(torch.ones(layers_to_aggregate))\n",
    "        self.layer_norms = nn.ModuleList(\n",
    "            [nn.LayerNorm(self.ssl_hidden_size) for _ in range(layers_to_aggregate)]\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Attention pooling for frame-level features\n",
    "        self.frame_pooling = AttentionPoolingLayer(embed_dim=self.ssl_hidden_size)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.ssl_hidden_size, self.ssl_hidden_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.ssl_hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # initialize weights of classifier\n",
    "        for name, param in self.classifier.named_parameters():\n",
    "            if 'weight' in name and len(param.shape) > 1:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_values = batch['input_values']  # (bs, seq_len)\n",
    "        \n",
    "        ssl_hidden_states = self.ssl_model(\n",
    "            input_values=input_values,\n",
    "            return_dict=True,\n",
    "        ).hidden_states  # tuple of (bs, seq_len, hidden_size)\n",
    "\n",
    "        # Weighted sum of all hidden layers\n",
    "        ssl_hidden_state = torch.zeros_like(ssl_hidden_states[-1])\n",
    "        weights = self.softmax(self.layer_weights)\n",
    "        for i in range(len(ssl_hidden_states)):\n",
    "            ssl_hidden_state += weights[i] * self.layer_norms[i](ssl_hidden_states[i])\n",
    "\n",
    "        # Attention pool over sequence length (frames)\n",
    "        pooled = self.frame_pooling(ssl_hidden_state, mask=None)  # (bs, hidden_size)\n",
    "        output = self.classifier(pooled)  # (bs, num_classes)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efceb32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/wav2vec2-base\"\n",
    "num_classes = 2 \n",
    "dataset_name = \"datasets/DAIC-WOZ-Cleaned-Split\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(dataset_name, 'train_split_Depression_AVEC2017.csv'))\n",
    "dev_df = pd.read_csv(os.path.join(dataset_name, 'dev_split_Depression_AVEC2017.csv'))\n",
    "\n",
    "train_paths, y_train = get_split_audio_paths(train_df, dataset_name)\n",
    "dev_paths, y_dev = get_split_audio_paths(dev_df, dataset_name)\n",
    "\n",
    "# Print class distribution for 10s segments\n",
    "print(\"Train 10s segment distribution:\", np.bincount(y_train))\n",
    "print(\"Dev 10s segment distribution:\", np.bincount(y_dev))\n",
    "train_counts = np.bincount(y_train)\n",
    "dev_counts = np.bincount(y_dev)\n",
    "print(\"Train 10s segment distribution:\", train_counts)\n",
    "print(\"Train 10s segment percentages:\", np.round(100 * train_counts / train_counts.sum(), 2), \"%\")\n",
    "print(\"Dev 10s segment distribution:\", dev_counts)\n",
    "print(\"Dev 10s segment percentages:\", np.round(100 * dev_counts / dev_counts.sum(), 2), \"%\")\n",
    "\n",
    "# Datasets\n",
    "train_dataset = AudioDepressionDataset(\n",
    "    audio_paths=train_paths,\n",
    "    labels=y_train,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "dev_dataset = AudioDepressionDataset(\n",
    "    audio_paths=dev_paths,\n",
    "    labels=y_dev,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# Modello\n",
    "model = DepressionClassifier(\n",
    "    model_name=model_name,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Ottimizzatore e loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\n=== Model Summary ===\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eaf247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "model_save_path = \"depression_classifier_best.pth\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_pbar = tqdm(enumerate(train_dataloader), \n",
    "                      total=len(train_dataloader),\n",
    "                      desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in train_pbar:\n",
    "        batch['input_values'] = batch['input_values'].to(device)\n",
    "        batch['label'] = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch['label'])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dev_dataloader:\n",
    "            batch['input_values'] = batch['input_values'].to(device)\n",
    "            batch['label'] = batch['label'].to(device)\n",
    "            \n",
    "            output = model(batch)\n",
    "            val_loss += criterion(output, batch['label']).item()\n",
    "            \n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            total += batch['label'].size(0)\n",
    "            correct += (predictions == batch['label']).sum().item()\n",
    "    \n",
    "    val_accuracy = correct / total\n",
    "    avg_val_loss = val_loss / len(dev_dataloader)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1: {val_f1:.4f}')\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved to {model_save_path} with F1: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load and prepare test set only after training/validation\n",
    "test_df = pd.read_csv(os.path.join(split_dataset_name, 'full_test_split.csv'))\n",
    "test_paths, y_test = get_split_audio_paths(test_df, split_dataset_name)\n",
    "\n",
    "test_dataset = AudioDepressionDataset(\n",
    "    audio_paths=test_paths,\n",
    "    labels=y_test,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
